{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import lmdb\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import kornia\n",
    "from skimage.color import lab2rgb\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "\n",
    "from scipy import signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def register(model, dummy_input_list):\n",
    "    # reset params\n",
    "    global registered_hooks, model_info_lst\n",
    "    registered_hooks, model_info_lst = [], []\n",
    "\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # forward\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(*dummy_input_list)\n",
    "\n",
    "    # remove hooks\n",
    "    for hook in registered_hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def parse_model_info(model):\n",
    "    tot_gflops = 0\n",
    "    for module_info in model_info_lst:\n",
    "        if module_info['gflops']:\n",
    "            tot_gflops += module_info['gflops']\n",
    "\n",
    "    tot_params = 0\n",
    "    for param in model.parameters():\n",
    "        tot_params += torch.prod(torch.tensor(param.size())).item()\n",
    "\n",
    "    return tot_gflops, tot_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_to_rgb(L, ab):\n",
    "    \"\"\"\n",
    "    Takes a batch of images\n",
    "    \"\"\"\n",
    "\n",
    "    L = (L + 1.) * 50.\n",
    "    ab = ab * 110.\n",
    "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
    "    rgb_imgs = []\n",
    "    for img in Lab:\n",
    "        img_rgb = lab2rgb(img)\n",
    "        rgb_imgs.append(img_rgb)\n",
    "    return np.stack(rgb_imgs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def create_kernel(sigma, ksize=None):\n",
    "    if ksize is None:\n",
    "        ksize = 1 + 2 * int(sigma * 3.0)\n",
    "\n",
    "    gkern1d = signal.gaussian(ksize, std=sigma).reshape(ksize, 1)\n",
    "    gkern2d = np.outer(gkern1d, gkern1d)\n",
    "    gaussian_kernel = gkern2d / gkern2d.sum()\n",
    "    zero_kernel = np.zeros_like(gaussian_kernel)\n",
    "\n",
    "    kernel = np.float32([\n",
    "        [gaussian_kernel, zero_kernel, zero_kernel],\n",
    "        [zero_kernel, gaussian_kernel, zero_kernel],\n",
    "        [zero_kernel, zero_kernel, gaussian_kernel]])\n",
    "\n",
    "    kernel = torch.from_numpy(kernel)\n",
    "\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def downsample_bd(data, kernel, scale, pad_data):\n",
    "    \"\"\"\n",
    "        Note:\n",
    "            1. `data` should be torch.FloatTensor (data range 0~1) in shape [nchw]\n",
    "            2. `pad_data` should be enabled in model testing\n",
    "            3. This function is device agnostic, i.e., data/kernel could be on cpu or gpu\n",
    "    \"\"\"\n",
    "\n",
    "    if pad_data:\n",
    "        # compute padding params\n",
    "        kernel_h, kernel_w = kernel.shape[-2:]\n",
    "        pad_h, pad_w = kernel_h - 1, kernel_w - 1\n",
    "        pad_t = pad_h // 2\n",
    "        pad_b = pad_h - pad_t\n",
    "        pad_l = pad_w // 2\n",
    "        pad_r = pad_w - pad_l\n",
    "\n",
    "        # pad data\n",
    "        data = F.pad(data, (pad_l, pad_r, pad_t, pad_b), 'reflect')\n",
    "\n",
    "    # blur + down sample\n",
    "    data = F.conv2d(data, kernel, stride=scale, bias=None, padding=0)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def get_dist_info():\n",
    "    \"\"\" Adopted from BasicSR\n",
    "    \"\"\"\n",
    "    if dist.is_available():\n",
    "        initialized = dist.is_initialized()\n",
    "    else:\n",
    "        initialized = False\n",
    "\n",
    "    if initialized:\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "    else:\n",
    "        rank = 0\n",
    "        world_size = 1\n",
    "\n",
    "    return rank, world_size\n",
    "\n",
    "\n",
    "def master_only(func):\n",
    "    \"\"\" Adopted from BasicSR\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        rank, _ = get_dist_info()\n",
    "        if rank == 0:\n",
    "            return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel():\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.scale = opt['scale']\n",
    "        self.device = torch.device(opt['device'])\n",
    "        self.blur_kernel = None\n",
    "        self.dist = opt['dist']\n",
    "        self.is_train = opt['is_train']\n",
    "\n",
    "        if self.is_train:\n",
    "            self.lr_data, self.gt_data = None, None\n",
    "            self.ckpt_dir = opt['train']['ckpt_dir']\n",
    "            self.log_decay = opt['logger'].get('decay', 0.99)\n",
    "            self.log_dict = OrderedDict()\n",
    "            self.running_log_dict = OrderedDict()\n",
    "\n",
    "    def set_networks(self):\n",
    "        pass\n",
    "\n",
    "    def set_criterions(self):\n",
    "        pass\n",
    "\n",
    "    def set_optimizers(self):\n",
    "        pass\n",
    "\n",
    "    def set_lr_schedules(self):\n",
    "        pass\n",
    "\n",
    "    def prepare_training_data(self, data):\n",
    "        \"\"\" prepare gt, lr data for training\n",
    "\n",
    "            for BD degradation, generate lr data and remove the border of gt data\n",
    "            for BI degradation, use input data directly\n",
    "        \"\"\"\n",
    "\n",
    "        degradation_type = self.opt['dataset']['degradation']['type']\n",
    "\n",
    "        if degradation_type == 'BI':\n",
    "            self.gt_data = data['gt'].to(self.device)\n",
    "            self.lr_data = data['lr'].to(self.device)\n",
    "\n",
    "        elif degradation_type == 'BD':\n",
    "            # generate lr data on the fly (on gpu)\n",
    "\n",
    "            # set params\n",
    "            scale = self.opt['scale']\n",
    "            sigma = self.opt['dataset']['degradation'].get('sigma', 1.5)\n",
    "            border_size = int(sigma * 3.0)\n",
    "\n",
    "            gt_data = data['gt'].to(self.device)  # with border\n",
    "            n, t, c, gt_h, gt_w = gt_data.size()\n",
    "            lr_h = (gt_h - 2*border_size)//scale\n",
    "            lr_w = (gt_w - 2*border_size)//scale\n",
    "\n",
    "            # create blurring kernel\n",
    "            if self.blur_kernel is None:\n",
    "                self.blur_kernel = create_kernel(sigma).to(self.device)\n",
    "            blur_kernel = self.blur_kernel\n",
    "\n",
    "            # generate lr data\n",
    "            gt_data = gt_data.view(n*t, c, gt_h, gt_w)\n",
    "            lr_data = downsample_bd(gt_data, blur_kernel, scale, pad_data=False)\n",
    "            lr_data = lr_data.view(n, t, c, lr_h, lr_w)\n",
    "\n",
    "            # remove gt border\n",
    "            gt_data = gt_data[\n",
    "                ...,\n",
    "                border_size: border_size + scale*lr_h,\n",
    "                border_size: border_size + scale*lr_w]\n",
    "            gt_data = gt_data.view(n, t, c, scale*lr_h, scale*lr_w)\n",
    "\n",
    "            self.gt_data, self.lr_data = gt_data, lr_data  # tchw|float32\n",
    "\n",
    "    def prepare_inference_data(self, data):\n",
    "        \"\"\" Prepare lr data for training (w/o loading on device)\n",
    "        \"\"\"\n",
    "\n",
    "        degradation_type = self.opt['dataset']['degradation']['type']\n",
    "\n",
    "        if degradation_type == 'BI':\n",
    "            self.lr_data = data['lr']\n",
    "\n",
    "        elif degradation_type == 'BD':\n",
    "            if 'lr' in data:\n",
    "                self.lr_data = data['lr']\n",
    "            else:\n",
    "                # generate lr data on the fly (on cpu)\n",
    "                # TODO: do frame-wise downsampling on gpu for acceleration?\n",
    "                gt_data = data['gt']  # thwc|uint8\n",
    "\n",
    "                # set params\n",
    "                scale = self.opt['scale']\n",
    "                sigma = self.opt['dataset']['degradation'].get('sigma', 1.5)\n",
    "\n",
    "                # create blurring kernel\n",
    "                if self.blur_kernel is None:\n",
    "                    self.blur_kernel = create_kernel(sigma)\n",
    "                blur_kernel = self.blur_kernel.cpu()\n",
    "\n",
    "                # generate lr data\n",
    "                gt_data = gt_data.permute(0, 3, 1, 2).float()  / 255.0  # tchw|float32\n",
    "                lr_data = downsample_bd(\n",
    "                    gt_data, blur_kernel, scale, pad_data=True)\n",
    "                lr_data = lr_data.permute(0, 2, 3, 1)  # thwc|float32\n",
    "\n",
    "                self.lr_data = lr_data\n",
    "\n",
    "        # thwc to tchw\n",
    "        self.lr_data = self.lr_data.permute(0, 3, 1, 2)  # tchw|float32\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def infer(self):\n",
    "        pass\n",
    "\n",
    "    def model_to_device(self, net):\n",
    "        net = net.to(self.device)\n",
    "        if self.dist:\n",
    "            net = nn.SyncBatchNorm.convert_sync_batchnorm(net)\n",
    "            net = DistributedDataParallel(\n",
    "                net, device_ids=[torch.cuda.current_device()])\n",
    "        return net\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        if hasattr(self, 'sched_G') and self.sched_G is not None:\n",
    "            self.sched_G.step()\n",
    "\n",
    "        if hasattr(self, 'sched_D') and self.sched_D is not None:\n",
    "            self.sched_D.step()\n",
    "\n",
    "    def get_learning_rate(self):\n",
    "        lr_dict = OrderedDict()\n",
    "\n",
    "        if hasattr(self, 'optim_G'):\n",
    "            lr_dict['lr_G'] = self.optim_G.param_groups[0]['lr']\n",
    "\n",
    "        if hasattr(self, 'optim_D'):\n",
    "            lr_dict['lr_D'] = self.optim_D.param_groups[0]['lr']\n",
    "\n",
    "        return lr_dict\n",
    "\n",
    "    def reduce_log(self):\n",
    "        if self.dist:\n",
    "            rank, world_size = self.opt['rank'], self.opt['world_size']\n",
    "            with torch.no_grad():\n",
    "                keys, vals = [], []\n",
    "                for key, val in self.log_dict.items():\n",
    "                    keys.append(key)\n",
    "                    vals.append(val)\n",
    "                vals = torch.FloatTensor(vals).to(self.device)\n",
    "                dist.reduce(vals, dst=0)\n",
    "                if rank == 0:  # average\n",
    "                    vals /= world_size\n",
    "                self.log_dict = {key: val.item() for key, val in zip(keys, vals)}\n",
    "\n",
    "    def update_running_log(self):\n",
    "        self.reduce_log()  # for distributed training\n",
    "\n",
    "        d = self.log_decay\n",
    "        for k in self.log_dict.keys():\n",
    "            current_val = self.log_dict[k]\n",
    "            running_val = self.running_log_dict.get(k)\n",
    "\n",
    "            if running_val is None:\n",
    "                running_val = current_val\n",
    "            else:\n",
    "                running_val = d * running_val + (1.0 - d) * current_val\n",
    "\n",
    "            self.running_log_dict[k] = running_val\n",
    "\n",
    "    def get_current_log(self):\n",
    "        return self.log_dict\n",
    "\n",
    "    def get_running_log(self):\n",
    "        return self.running_log_dict\n",
    "\n",
    "    def get_format_msg(self, epoch, iter):\n",
    "        # generic info\n",
    "        msg = f'[epoch: {epoch} | iter: {iter}'\n",
    "        for lr_type, lr in self.get_learning_rate().items():\n",
    "            msg += f' | {lr_type}: {lr:.2e}'\n",
    "        msg += '] '\n",
    "\n",
    "        # loss info\n",
    "        log_dict = self.get_running_log()\n",
    "        msg += ', '.join([f'{k}: {v:.3e}' for k, v in log_dict.items()])\n",
    "\n",
    "        return msg\n",
    "\n",
    "    def save(self, current_iter):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bare_model(net):\n",
    "        if isinstance(net, DistributedDataParallel):\n",
    "            net = net.module\n",
    "        return net\n",
    "\n",
    "    @master_only\n",
    "    def save_network(self, net, net_label, current_iter):\n",
    "        filename = f'{net_label}_iter{current_iter}.pth'\n",
    "        save_path = osp.join(self.ckpt_dir, filename)\n",
    "        net = self.get_bare_model(net)\n",
    "        torch.save(net.state_dict(), save_path)\n",
    "\n",
    "    def save_training_state(self, current_epoch, current_iter):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def load_network(self, net, load_path):\n",
    "        state_dict = torch.load(\n",
    "            load_path, map_location=lambda storage, loc: storage)\n",
    "        net = self.get_bare_model(net)\n",
    "        net.load_state_dict(state_dict)\n",
    "\n",
    "    def pad_sequence(self, lr_data):\n",
    "        \"\"\"\n",
    "            Parameters:\n",
    "                :param lr_data: tensor in shape tchw\n",
    "        \"\"\"\n",
    "        padding_mode = self.opt['test'].get('padding_mode', 'reflect')\n",
    "        n_pad_front = self.opt['test'].get('num_pad_front', 0)\n",
    "        assert n_pad_front < lr_data.size(0)\n",
    "\n",
    "        # pad\n",
    "        if padding_mode == 'reflect':\n",
    "            lr_data = torch.cat(\n",
    "                [lr_data[1: 1 + n_pad_front, ...].flip(0), lr_data], dim=0)\n",
    "\n",
    "        elif padding_mode == 'replicate':\n",
    "            lr_data = torch.cat(\n",
    "                [lr_data[:1, ...].expand(n_pad_front, -1, -1, -1), lr_data], dim=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f'Unrecognized padding mode: {padding_mode}')\n",
    "\n",
    "        return lr_data, n_pad_front\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel():\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.scale = opt['scale']\n",
    "        self.device = torch.device(opt['device'])\n",
    "        self.blur_kernel = None\n",
    "        self.dist = opt['dist']\n",
    "        self.is_train = opt['is_train']\n",
    "\n",
    "        if self.is_train:\n",
    "            self.lr_data, self.gt_data = None, None\n",
    "            self.ckpt_dir = opt['train']['ckpt_dir']\n",
    "            self.log_decay = opt['logger'].get('decay', 0.99)\n",
    "            self.log_dict = OrderedDict()\n",
    "            self.running_log_dict = OrderedDict()\n",
    "\n",
    "    def set_networks(self):\n",
    "        pass\n",
    "\n",
    "    def set_criterions(self):\n",
    "        pass\n",
    "\n",
    "    def set_optimizers(self):\n",
    "        pass\n",
    "\n",
    "    def set_lr_schedules(self):\n",
    "        pass\n",
    "\n",
    "    def prepare_training_data(self, data):\n",
    "        \"\"\" prepare gt, lr data for training\n",
    "\n",
    "            for BD degradation, generate lr data and remove the border of gt data\n",
    "            for BI degradation, use input data directly\n",
    "        \"\"\"\n",
    "\n",
    "        degradation_type = self.opt['dataset']['degradation']['type']\n",
    "\n",
    "        if degradation_type == 'BI':\n",
    "            self.gt_data = data['gt'].to(self.device)\n",
    "            self.lr_data = data['lr'].to(self.device)\n",
    "\n",
    "        elif degradation_type == 'BD':\n",
    "            # generate lr data on the fly (on gpu)\n",
    "\n",
    "            # set params\n",
    "            scale = self.opt['scale']\n",
    "            sigma = self.opt['dataset']['degradation'].get('sigma', 1.5)\n",
    "            border_size = int(sigma * 3.0)\n",
    "\n",
    "            gt_data = data['gt'].to(self.device)  # with border\n",
    "            n, t, c, gt_h, gt_w = gt_data.size()\n",
    "            lr_h = (gt_h - 2*border_size)//scale\n",
    "            lr_w = (gt_w - 2*border_size)//scale\n",
    "\n",
    "            # create blurring kernel\n",
    "            if self.blur_kernel is None:\n",
    "                self.blur_kernel = create_kernel(sigma).to(self.device)\n",
    "            blur_kernel = self.blur_kernel\n",
    "\n",
    "            # generate lr data\n",
    "            gt_data = gt_data.view(n*t, c, gt_h, gt_w)\n",
    "            lr_data = downsample_bd(gt_data, blur_kernel, scale, pad_data=False)\n",
    "            lr_data = lr_data.view(n, t, c, lr_h, lr_w)\n",
    "\n",
    "            # remove gt border\n",
    "            gt_data = gt_data[\n",
    "                ...,\n",
    "                border_size: border_size + scale*lr_h,\n",
    "                border_size: border_size + scale*lr_w]\n",
    "            gt_data = gt_data.view(n, t, c, scale*lr_h, scale*lr_w)\n",
    "\n",
    "            self.gt_data, self.lr_data = gt_data, lr_data  # tchw|float32\n",
    "\n",
    "    def prepare_inference_data(self, data):\n",
    "        \"\"\" Prepare lr data for training (w/o loading on device)\n",
    "        \"\"\"\n",
    "\n",
    "        degradation_type = self.opt['dataset']['degradation']['type']\n",
    "\n",
    "        if degradation_type == 'BI':\n",
    "            self.lr_data = data['lr']\n",
    "\n",
    "        elif degradation_type == 'BD':\n",
    "            if 'lr' in data:\n",
    "                self.lr_data = data['lr']\n",
    "            else:\n",
    "                # generate lr data on the fly (on cpu)\n",
    "                # TODO: do frame-wise downsampling on gpu for acceleration?\n",
    "                gt_data = data['gt']  # thwc|uint8\n",
    "\n",
    "                # set params\n",
    "                scale = self.opt['scale']\n",
    "                sigma = self.opt['dataset']['degradation'].get('sigma', 1.5)\n",
    "\n",
    "                # create blurring kernel\n",
    "                if self.blur_kernel is None:\n",
    "                    self.blur_kernel = create_kernel(sigma)\n",
    "                blur_kernel = self.blur_kernel.cpu()\n",
    "\n",
    "                # generate lr data\n",
    "                gt_data = gt_data.permute(0, 3, 1, 2).float()  / 255.0  # tchw|float32\n",
    "                lr_data = downsample_bd(\n",
    "                    gt_data, blur_kernel, scale, pad_data=True)\n",
    "                lr_data = lr_data.permute(0, 2, 3, 1)  # thwc|float32\n",
    "\n",
    "                self.lr_data = lr_data\n",
    "\n",
    "        # thwc to tchw\n",
    "        self.lr_data = self.lr_data.permute(0, 3, 1, 2)  # tchw|float32\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def infer(self):\n",
    "        pass\n",
    "\n",
    "    def model_to_device(self, net):\n",
    "        net = net.to(self.device)\n",
    "        if self.dist:\n",
    "            net = nn.SyncBatchNorm.convert_sync_batchnorm(net)\n",
    "            net = DistributedDataParallel(\n",
    "                net, device_ids=[torch.cuda.current_device()])\n",
    "        return net\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        if hasattr(self, 'sched_G') and self.sched_G is not None:\n",
    "            self.sched_G.step()\n",
    "\n",
    "        if hasattr(self, 'sched_D') and self.sched_D is not None:\n",
    "            self.sched_D.step()\n",
    "\n",
    "    def get_learning_rate(self):\n",
    "        lr_dict = OrderedDict()\n",
    "\n",
    "        if hasattr(self, 'optim_G'):\n",
    "            lr_dict['lr_G'] = self.optim_G.param_groups[0]['lr']\n",
    "\n",
    "        if hasattr(self, 'optim_D'):\n",
    "            lr_dict['lr_D'] = self.optim_D.param_groups[0]['lr']\n",
    "\n",
    "        return lr_dict\n",
    "\n",
    "    def reduce_log(self):\n",
    "        if self.dist:\n",
    "            rank, world_size = self.opt['rank'], self.opt['world_size']\n",
    "            with torch.no_grad():\n",
    "                keys, vals = [], []\n",
    "                for key, val in self.log_dict.items():\n",
    "                    keys.append(key)\n",
    "                    vals.append(val)\n",
    "                vals = torch.FloatTensor(vals).to(self.device)\n",
    "                dist.reduce(vals, dst=0)\n",
    "                if rank == 0:  # average\n",
    "                    vals /= world_size\n",
    "                self.log_dict = {key: val.item() for key, val in zip(keys, vals)}\n",
    "\n",
    "    def update_running_log(self):\n",
    "        self.reduce_log()  # for distributed training\n",
    "\n",
    "        d = self.log_decay\n",
    "        for k in self.log_dict.keys():\n",
    "            current_val = self.log_dict[k]\n",
    "            running_val = self.running_log_dict.get(k)\n",
    "\n",
    "            if running_val is None:\n",
    "                running_val = current_val\n",
    "            else:\n",
    "                running_val = d * running_val + (1.0 - d) * current_val\n",
    "\n",
    "            self.running_log_dict[k] = running_val\n",
    "\n",
    "    def get_current_log(self):\n",
    "        return self.log_dict\n",
    "\n",
    "    def get_running_log(self):\n",
    "        return self.running_log_dict\n",
    "\n",
    "    def get_format_msg(self, epoch, iter):\n",
    "        # generic info\n",
    "        msg = f'[epoch: {epoch} | iter: {iter}'\n",
    "        for lr_type, lr in self.get_learning_rate().items():\n",
    "            msg += f' | {lr_type}: {lr:.2e}'\n",
    "        msg += '] '\n",
    "\n",
    "        # loss info\n",
    "        log_dict = self.get_running_log()\n",
    "        msg += ', '.join([f'{k}: {v:.3e}' for k, v in log_dict.items()])\n",
    "\n",
    "        return msg\n",
    "\n",
    "    def save(self, current_iter):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bare_model(net):\n",
    "        if isinstance(net, DistributedDataParallel):\n",
    "            net = net.module\n",
    "        return net\n",
    "\n",
    "    @master_only\n",
    "    def save_network(self, net, net_label, current_iter):\n",
    "        filename = f'{net_label}_iter{current_iter}.pth'\n",
    "        save_path = osp.join(self.ckpt_dir, filename)\n",
    "        net = self.get_bare_model(net)\n",
    "        torch.save(net.state_dict(), save_path)\n",
    "\n",
    "    def save_training_state(self, current_epoch, current_iter):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def load_network(self, net, load_path):\n",
    "        state_dict = torch.load(\n",
    "            load_path, map_location=lambda storage, loc: storage)\n",
    "        net = self.get_bare_model(net)\n",
    "        net.load_state_dict(state_dict)\n",
    "\n",
    "    def pad_sequence(self, lr_data):\n",
    "        \"\"\"\n",
    "            Parameters:\n",
    "                :param lr_data: tensor in shape tchw\n",
    "        \"\"\"\n",
    "        padding_mode = self.opt['test'].get('padding_mode', 'reflect')\n",
    "        n_pad_front = self.opt['test'].get('num_pad_front', 0)\n",
    "        assert n_pad_front < lr_data.size(0)\n",
    "\n",
    "        # pad\n",
    "        if padding_mode == 'reflect':\n",
    "            lr_data = torch.cat(\n",
    "                [lr_data[1: 1 + n_pad_front, ...].flip(0), lr_data], dim=0)\n",
    "\n",
    "        elif padding_mode == 'replicate':\n",
    "            lr_data = torch.cat(\n",
    "                [lr_data[:1, ...].expand(n_pad_front, -1, -1, -1), lr_data], dim=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f'Unrecognized padding mode: {padding_mode}')\n",
    "\n",
    "        return lr_data, n_pad_front\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===----------------- utility functions -------------------- #\n",
    "def initialize_weights(net_l, init_type='kaiming', scale=1):\n",
    "    \"\"\" Modify from BasicSR/MMSR\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(net_l, list):\n",
    "        net_l = [net_l]\n",
    "\n",
    "    for net in net_l:\n",
    "        for m in net.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                if init_type == 'xavier':\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                elif init_type == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
    "                else:\n",
    "                    raise NotImplementedError(init_type)\n",
    "\n",
    "                m.weight.data *= scale  # to stabilize training\n",
    "\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight.data, 1)\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "def space_to_depth(x, scale):\n",
    "    \"\"\" Equivalent to tf.space_to_depth()\n",
    "    \"\"\"\n",
    "\n",
    "    n, c, in_h, in_w = x.size()\n",
    "    out_h, out_w = in_h // scale, in_w // scale\n",
    "\n",
    "    x_reshaped = x.reshape(n, c, out_h, scale, out_w, scale)\n",
    "    x_reshaped = x_reshaped.permute(0, 3, 5, 1, 2, 4)\n",
    "    output = x_reshaped.reshape(n, scale * scale * c, out_h, out_w)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def backward_warp(x, flow, mode='bilinear', padding_mode='border'):\n",
    "    \"\"\" Backward warp `x` according to `flow`\n",
    "\n",
    "        Both x and flow are pytorch tensor in shape `nchw` and `n2hw`\n",
    "\n",
    "        Reference:\n",
    "            https://github.com/sniklaus/pytorch-spynet/blob/master/run.py#L41\n",
    "    \"\"\"\n",
    "\n",
    "    n, c, h, w = x.size()\n",
    "\n",
    "    # create mesh grid\n",
    "    iu = torch.linspace(-1.0, 1.0, w).view(1, 1, 1, w).expand(n, -1, h, -1)\n",
    "    iv = torch.linspace(-1.0, 1.0, h).view(1, 1, h, 1).expand(n, -1, -1, w)\n",
    "    grid = torch.cat([iu, iv], 1).to(flow.device)\n",
    "\n",
    "    # normalize flow to [-1, 1]\n",
    "    flow = torch.cat([\n",
    "        flow[:, 0:1, ...] / ((w - 1.0) / 2.0),\n",
    "        flow[:, 1:2, ...] / ((h - 1.0) / 2.0)], dim=1)\n",
    "\n",
    "    # add flow to grid and reshape to nhw2\n",
    "    grid = (grid + flow).permute(0, 2, 3, 1)\n",
    "\n",
    "    # bilinear sampling\n",
    "    # Note: `align_corners` is set to `True` by default for PyTorch version < 1.4.0\n",
    "    if int(''.join(torch.__version__.split('.')[:2])) >= 14:\n",
    "        output = F.grid_sample(\n",
    "            x, grid, mode=mode, padding_mode=padding_mode, align_corners=True)\n",
    "    else:\n",
    "        output = F.grid_sample(x, grid, mode=mode, padding_mode=padding_mode)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_upsampling_func(scale=4, degradation='BI'):\n",
    "    if degradation == 'BI':\n",
    "        upsample_func = functools.partial(\n",
    "            F.interpolate, scale_factor=scale, mode='bilinear',\n",
    "            align_corners=False)\n",
    "\n",
    "    elif degradation == 'BD':\n",
    "        upsample_func = BicubicUpsampler(scale_factor=scale)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Unrecognized degradation type: {degradation}')\n",
    "\n",
    "    return upsample_func\n",
    "\n",
    "\n",
    "# --------------------- utility classes --------------------- #\n",
    "class BicubicUpsampler(nn.Module):\n",
    "    \"\"\" Bicubic upsampling function with similar behavior to that in TecoGAN-Tensorflow\n",
    "\n",
    "        Note:\n",
    "            This function is different from torch.nn.functional.interpolate and matlab's imresize\n",
    "            in terms of the bicubic kernel and the sampling strategy\n",
    "\n",
    "        References:\n",
    "            http://verona.fi-p.unam.mx/boris/practicas/CubConvInterp.pdf\n",
    "            https://stackoverflow.com/questions/26823140/imresize-trying-to-understand-the-bicubic-interpolation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale_factor, a=-0.75):\n",
    "        super(BicubicUpsampler, self).__init__()\n",
    "\n",
    "        # calculate weights (according to Eq.(6) in the reference paper)\n",
    "        cubic = torch.FloatTensor([\n",
    "            [0, a, -2*a, a],\n",
    "            [1, 0, -(a + 3), a + 2],\n",
    "            [0, -a, (2*a + 3), -(a + 2)],\n",
    "            [0, 0, a, -a]\n",
    "        ])\n",
    "\n",
    "        kernels = [\n",
    "            torch.matmul(cubic, torch.FloatTensor([1, s, s**2, s**3]))\n",
    "            for s in [1.0*d/scale_factor for d in range(scale_factor)]\n",
    "        ]  # s = x - floor(x)\n",
    "\n",
    "        # register parameters\n",
    "        self.scale_factor = scale_factor\n",
    "        self.register_buffer('kernels', torch.stack(kernels))  # size: (f, 4)\n",
    "\n",
    "    def forward(self, input):\n",
    "        n, c, h, w = input.size()\n",
    "        f = self.scale_factor\n",
    "\n",
    "        # merge n&c\n",
    "        input = input.reshape(n*c, 1, h, w)\n",
    "\n",
    "        # pad input (left, right, top, bottom)\n",
    "        input = F.pad(input, (1, 2, 1, 2), mode='replicate')\n",
    "\n",
    "        # calculate output (vertical expansion)\n",
    "        kernel_h = self.kernels.view(f, 1, 4, 1)\n",
    "        output = F.conv2d(input, kernel_h, stride=1, padding=0)\n",
    "        output = output.permute(0, 2, 1, 3).reshape(n*c, 1, f*h, w + 3)\n",
    "\n",
    "        # calculate output (horizontal expansion)\n",
    "        kernel_w = self.kernels.view(f, 1, 1, 4)\n",
    "        output = F.conv2d(output, kernel_w, stride=1, padding=0)\n",
    "        output = output.permute(0, 2, 3, 1).reshape(n*c, 1, f*h, f*w)\n",
    "\n",
    "        # split n&c\n",
    "        output = output.reshape(n, c, f*h, f*w)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSequenceGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseSequenceGenerator, self).__init__()\n",
    "\n",
    "    def generate_dummy_data(self, lr_size):\n",
    "        \"\"\" Generate random input tensors for function `step`\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def profile(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\" Interface (support DDP)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward_sequence(self, lr_data):\n",
    "        \"\"\" Forward a whole sequence (for training)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        \"\"\" Forward a single frame\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def infer_sequence(self, lr_data, device):\n",
    "        \"\"\" Infer a whole sequence (for inference)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class BaseSequenceDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseSequenceDiscriminator, self).__init__()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\" Interface (support DDP)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        \"\"\" Forward a singe frame\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward_sequence(self, data, args_dict):\n",
    "        \"\"\" Forward a whole sequence (for training)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self, feature_indexs=(8, 17, 26, 35)):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "\n",
    "        # init feature layers\n",
    "        self.features = torchvision.models.vgg19(pretrained=True).features\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Notes:\n",
    "        # 1. default feature layers are 8(conv2_2), 17(conv3_4), 26(conv4_4),\n",
    "        #    35(conv5_4)\n",
    "        # 2. features are extracted after ReLU activation\n",
    "        self.feature_indexs = sorted(feature_indexs)\n",
    "\n",
    "        # register normalization params\n",
    "        mean = torch.FloatTensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)  # RGB\n",
    "        std  = torch.FloatTensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "        self.register_buffer('mean', mean)\n",
    "        self.register_buffer('std', std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # assume input ranges in [0, 1]\n",
    "        out = (x - self.mean) / self.std\n",
    "\n",
    "        feature_list = []\n",
    "        for i in range(len(self.features)):\n",
    "            out = self.features[i](out)\n",
    "            if i in self.feature_indexs:\n",
    "                # clone to prevent overlapping by inplaced ReLU\n",
    "                feature_list.append(out.clone())\n",
    "\n",
    "        return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float32_to_uint8(inputs):\n",
    "    \"\"\" Convert np.float32 array to np.uint8\n",
    "\n",
    "        Parameters:\n",
    "            :param input: np.float32, (NT)CHW, [0, 1]\n",
    "            :return: np.uint8, (NT)CHW, [0, 255]\n",
    "    \"\"\"\n",
    "    return np.uint8(np.clip(np.round(inputs * 255), 0, 255))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================== generator modules ====================== #\n",
    "class FNet(nn.Module):\n",
    "    \"\"\" Optical flow estimation network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_nc):\n",
    "        super(FNet, self).__init__()\n",
    "\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(2*in_nc, 32, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool2d(2, 2))\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool2d(2, 2))\n",
    "\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool2d(2, 2))\n",
    "\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        self.flow = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 2, 3, 1, 1, bias=True))\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\" Compute optical flow from x1 to x2\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.encoder1(torch.cat([x1, x2], dim=1))\n",
    "        out = self.encoder2(out)\n",
    "        out = self.encoder3(out)\n",
    "        out = F.interpolate(\n",
    "            self.decoder1(out), scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        out = F.interpolate(\n",
    "            self.decoder2(out), scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        out = F.interpolate(\n",
    "            self.decoder3(out), scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        out = torch.tanh(self.flow(out)) * 24  # 24 is the max velocity\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\" Residual block without batch normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nf=64):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(nf, nf, 3, 1, 1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(nf, nf, 3, 1, 1, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x) + x\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SRNet(nn.Module):\n",
    "    \"\"\" Reconstruction & Upsampling network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_nc, out_nc, nf, nb, upsample_func, scale):\n",
    "        super(SRNet, self).__init__()\n",
    "\n",
    "        # input conv.\n",
    "        self.conv_in = nn.Sequential(\n",
    "            nn.Conv2d((scale**2 + 1) * in_nc, nf, 3, 1, 1, bias=True),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "        # residual blocks\n",
    "        self.resblocks = nn.Sequential(*[ResidualBlock(nf) for _ in range(nb)])\n",
    "\n",
    "        # upsampling blocks\n",
    "        conv_up = [\n",
    "            nn.ConvTranspose2d(nf, nf, 3, 2, 1, output_padding=1, bias=True),\n",
    "            nn.ReLU(inplace=True)]\n",
    "\n",
    "        if scale == 4:\n",
    "            conv_up += [\n",
    "                nn.ConvTranspose2d(nf, nf, 3, 2, 1, output_padding=1, bias=True),\n",
    "                nn.ReLU(inplace=True)]\n",
    "\n",
    "        self.conv_up = nn.Sequential(*conv_up)\n",
    "\n",
    "        # output conv.\n",
    "        self.conv_out = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)\n",
    "\n",
    "        # upsampling function\n",
    "        self.upsample_func = upsample_func\n",
    "\n",
    "    def forward(self, lr_curr, hr_prev_tran):\n",
    "        \"\"\" lr_curr: the current lr data in shape nchw\n",
    "            hr_prev_tran: the previous transformed hr_data in shape n(s*s*c)hw\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.conv_in(torch.cat([lr_curr, hr_prev_tran], dim=1))\n",
    "        out = self.resblocks(out)\n",
    "        out = self.conv_up(out)\n",
    "        out = self.conv_out(out)\n",
    "        out += self.upsample_func(lr_curr)\n",
    "\n",
    "        return out\n",
    "\n",
    "from fastai.vision.learner import create_body\n",
    "from torchvision.models.resnet import resnet18\n",
    "from fastai.vision.models.unet import DynamicUnet\n",
    "\n",
    "def build_res_unet(n_input=1, n_output=2, size=256):\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    body = create_body(resnet18(), pretrained=True, n_in=n_input, cut=-2)\n",
    "    #net_G = DynamicUnet(body, n_output, (size, size)).to(device)\n",
    "    net_G = DynamicUnet(body, n_output, (size, size))\n",
    "    return net_G\n",
    "\n",
    "class ColorUNet(nn.Module):\n",
    "    def __init__(self, in_nc, out_nc):\n",
    "        super(ColorUNet, self).__init__()\n",
    "\n",
    "        self.unet=build_res_unet(n_input=in_nc+2, n_output=out_nc, size=256)\n",
    "    \n",
    "    def forward(self, L, prev_ab):\n",
    "        x = torch.cat([L, prev_ab], dim=1)\n",
    "        return self.unet(x)\n",
    "\n",
    "class FRNet(BaseSequenceGenerator):\n",
    "    \"\"\" Frame-recurrent network: https://arxiv.org/abs/1801.04590\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_nc, out_nc):\n",
    "        super(FRNet, self).__init__()\n",
    "\n",
    "        # define fnet & srnet\n",
    "        self.fnet = FNet(in_nc)\n",
    "        self.srnet = ColorUNet(in_nc, out_nc)\n",
    "\n",
    "    def forward(self, lr_data, device=None):\n",
    "        if self.training:\n",
    "            out = self.forward_sequence(lr_data)\n",
    "        else:\n",
    "            out = self.infer_sequence(lr_data, device)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward_sequence(self, lr_data):\n",
    "        \"\"\"\n",
    "            Parameters:\n",
    "                :param lr_data: lr data in shape ntchw\n",
    "        \"\"\"\n",
    "\n",
    "        n, t, c, lr_h, lr_w = lr_data.size()\n",
    "        hr_h, hr_w = lr_h * self.scale, lr_w * self.scale\n",
    "\n",
    "        # calculate optical flows\n",
    "        lr_prev = lr_data[:, :-1, ...].reshape(n * (t - 1), c, lr_h, lr_w)\n",
    "        lr_curr = lr_data[:, 1:, ...].reshape(n * (t - 1), c, lr_h, lr_w)\n",
    "        lr_flow = self.fnet(lr_curr, lr_prev)  # n*(t-1),2,h,w\n",
    "\n",
    "        # upsample lr flows\n",
    "        hr_flow = self.scale * self.upsample_func(lr_flow)\n",
    "        hr_flow = hr_flow.view(n, (t - 1), 2, hr_h, hr_w)\n",
    "\n",
    "        # compute the first hr data\n",
    "        hr_data = []\n",
    "        hr_prev = self.srnet(\n",
    "            lr_data[:, 0, ...],\n",
    "            torch.zeros(n, (self.scale**2)*c, lr_h, lr_w, dtype=torch.float32,\n",
    "                        device=lr_data.device))\n",
    "        hr_data.append(hr_prev)\n",
    "\n",
    "        # compute the remaining hr data\n",
    "        for i in range(1, t):\n",
    "            # warp hr_prev\n",
    "            hr_prev_warp = backward_warp(hr_prev, hr_flow[:, i - 1, ...])\n",
    "\n",
    "            # compute hr_curr\n",
    "            hr_curr = self.srnet(\n",
    "                lr_data[:, i, ...],\n",
    "                space_to_depth(hr_prev_warp, self.scale))\n",
    "\n",
    "            # save and update\n",
    "            hr_data.append(hr_curr)\n",
    "            hr_prev = hr_curr\n",
    "\n",
    "        hr_data = torch.stack(hr_data, dim=1)  # n,t,c,hr_h,hr_w\n",
    "\n",
    "        # construct output dict\n",
    "        ret_dict = {\n",
    "            'hr_data': hr_data,  # n,t,c,hr_h,hr_w\n",
    "            'hr_flow': hr_flow,  # n,t,2,hr_h,hr_w\n",
    "            'lr_prev': lr_prev,  # n(t-1),c,lr_h,lr_w\n",
    "            'lr_curr': lr_curr,  # n(t-1),c,lr_h,lr_w\n",
    "            'lr_flow': lr_flow,  # n(t-1),2,lr_h,lr_w\n",
    "        }\n",
    "\n",
    "        return ret_dict\n",
    "\n",
    "    def step(self, lr_curr, lr_prev, hr_prev):\n",
    "        \"\"\"\n",
    "            Parameters:\n",
    "                :param lr_curr: the current lr data in shape nchw\n",
    "                :param lr_prev: the previous lr data in shape nchw\n",
    "                :param hr_prev: the previous hr data in shape nc(sh)(sw)\n",
    "        \"\"\"\n",
    "\n",
    "        # estimate lr flow (lr_curr -> lr_prev)\n",
    "        lr_flow = self.fnet(lr_curr, lr_prev)\n",
    "\n",
    "        # pad if size is not a multiple of 8\n",
    "        pad_h = lr_curr.size(2) - lr_curr.size(2)//8*8\n",
    "        pad_w = lr_curr.size(3) - lr_curr.size(3)//8*8\n",
    "        lr_flow_pad = F.pad(lr_flow, (0, pad_w, 0, pad_h), 'reflect')\n",
    "\n",
    "        # upsample lr flow\n",
    "        hr_flow = self.scale * self.upsample_func(lr_flow_pad)\n",
    "\n",
    "        # warp hr_prev\n",
    "        hr_prev_warp = backward_warp(hr_prev, hr_flow)\n",
    "\n",
    "        # compute hr_curr\n",
    "        hr_curr = self.srnet(lr_curr, space_to_depth(hr_prev_warp, self.scale))\n",
    "\n",
    "        return hr_curr\n",
    "\n",
    "    def infer_sequence(self, lr_data, device):\n",
    "        \"\"\"\n",
    "            Parameters:\n",
    "                :param lr_data: torch.FloatTensor in shape tchw\n",
    "                :param device: torch.device\n",
    "\n",
    "                :return hr_seq: uint8 np.ndarray in shape tchw\n",
    "        \"\"\"\n",
    "\n",
    "        # set params\n",
    "        tot_frm, c, h, w = lr_data.size()\n",
    "        s = self.scale\n",
    "\n",
    "        # forward\n",
    "        hr_seq = []\n",
    "        lr_prev = torch.zeros(1, c, h, w, dtype=torch.float32).to(device)\n",
    "        hr_prev = torch.zeros(1, c, s*h, s*w, dtype=torch.float32).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(tot_frm):\n",
    "                lr_curr = lr_data[i: i + 1, ...].to(device)\n",
    "                hr_curr = self.step(lr_curr, lr_prev, hr_prev)\n",
    "                lr_prev, hr_prev = lr_curr, hr_curr\n",
    "\n",
    "                hr_frm = hr_curr.squeeze(0).cpu().numpy()  # chw|rgb|uint8\n",
    "                hr_seq.append(float32_to_uint8(hr_frm))\n",
    "\n",
    "        return np.stack(hr_seq).transpose(0, 2, 3, 1)  # thwc\n",
    "\n",
    "    def generate_dummy_data(self, lr_size, device):\n",
    "        c, lr_h, lr_w = lr_size\n",
    "        s = self.scale\n",
    "\n",
    "        # generate dummy input data\n",
    "        lr_curr = torch.rand(1, c, lr_h, lr_w, dtype=torch.float32).to(device)\n",
    "        lr_prev = torch.rand(1, c, lr_h, lr_w, dtype=torch.float32).to(device)\n",
    "        hr_prev = torch.rand(1, c, s*lr_h, s*lr_w, dtype=torch.float32).to(device)\n",
    "\n",
    "        data_list = [lr_curr, lr_prev, hr_prev]\n",
    "        return data_list\n",
    "\n",
    "    def profile(self, lr_size, device):\n",
    "        gflops_dict, params_dict = OrderedDict(), OrderedDict()\n",
    "\n",
    "        # generate dummy input data\n",
    "        lr_curr, lr_prev, hr_prev = self.generate_dummy_data(lr_size, device)\n",
    "\n",
    "        # profile module 1: flow estimation module\n",
    "        lr_flow = register(self.fnet, [lr_curr, lr_prev])\n",
    "        gflops_dict['FNet'], params_dict['FNet'] = parse_model_info(self.fnet)\n",
    "\n",
    "        # profile module 2: sr module\n",
    "        pad_h = lr_curr.size(2) - lr_curr.size(2)//8*8\n",
    "        pad_w = lr_curr.size(3) - lr_curr.size(3)//8*8\n",
    "        lr_flow_pad = F.pad(lr_flow, (0, pad_w, 0, pad_h), 'reflect')\n",
    "        hr_flow = self.scale * self.upsample_func(lr_flow_pad)\n",
    "        hr_prev_warp = backward_warp(hr_prev, hr_flow)\n",
    "        _ = register(self.srnet, [lr_curr, space_to_depth(hr_prev_warp, self.scale)])\n",
    "        gflops_dict['SRNet'], params_dict['SRNet'] = parse_model_info(self.srnet)\n",
    "\n",
    "        return gflops_dict, params_dict\n",
    "\n",
    "\n",
    "# ====================== discriminator modules ====================== #\n",
    "class DiscriminatorBlocks(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorBlocks, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(  # /2\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        self.block2 = nn.Sequential(  # /4\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        self.block3 = nn.Sequential(  # /8\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        self.block4 = nn.Sequential(  # /16\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.block1(x)\n",
    "        out2 = self.block2(out1)\n",
    "        out3 = self.block3(out2)\n",
    "        out4 = self.block4(out3)\n",
    "        feature_list = [out1, out2, out3, out4]\n",
    "\n",
    "        return out4, feature_list\n",
    "\n",
    "\n",
    "class SpatioTemporalDiscriminator(BaseSequenceDiscriminator):\n",
    "    \"\"\" Spatio-Temporal discriminator proposed in TecoGAN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_nc, spatial_size, tempo_range, degradation, scale):\n",
    "        super(SpatioTemporalDiscriminator, self).__init__()\n",
    "\n",
    "        # basic settings\n",
    "        mult = 3  # (conditional triplet, input triplet, warped triplet)\n",
    "        self.spatial_size = spatial_size\n",
    "        self.tempo_range = tempo_range\n",
    "        assert self.tempo_range == 3, 'currently only support 3 as tempo_range'\n",
    "        self.scale = scale\n",
    "\n",
    "        # input conv.\n",
    "        self.conv_in = nn.Sequential(\n",
    "            nn.Conv2d(in_nc*tempo_range*mult, 64, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        # discriminator block\n",
    "        self.discriminator_block = DiscriminatorBlocks()  # downsample 16x\n",
    "\n",
    "        # classifier\n",
    "        self.dense = nn.Linear(256 * spatial_size // 16 * spatial_size // 16, 1)\n",
    "\n",
    "        # get upsampling function according to degradation type\n",
    "        self.upsample_func = get_upsampling_func(self.scale, degradation)\n",
    "\n",
    "    def forward(self, data, args_dict):\n",
    "        out = self.forward_sequence(data, args_dict)\n",
    "        return out\n",
    "\n",
    "    def forward_sequence(self, data, args_dict):\n",
    "        \"\"\"\n",
    "            :param data: should be either hr_data or gt_data\n",
    "            :param args_dict: a dict including data/config required here\n",
    "        \"\"\"\n",
    "\n",
    "        # === set params === #\n",
    "        net_G = args_dict['net_G']\n",
    "        lr_data = args_dict['lr_data']\n",
    "        bi_data = args_dict['bi_data']\n",
    "        hr_flow = args_dict['hr_flow']\n",
    "\n",
    "        n, t, c, lr_h, lr_w = lr_data.size()\n",
    "        _, _, _, hr_h, hr_w = data.size()\n",
    "\n",
    "        s_size = self.spatial_size\n",
    "        t = t // 3 * 3  # discard other frames\n",
    "        n_clip = n * t // 3  # total number of 3-frame clips in all batches\n",
    "\n",
    "        c_size = int(s_size * args_dict['crop_border_ratio'])\n",
    "        n_pad = (s_size - c_size) // 2\n",
    "\n",
    "        # === compute forward & backward flow === #\n",
    "        if 'hr_flow_merge' not in args_dict:\n",
    "            if args_dict['use_pp_crit']:\n",
    "                hr_flow_bw = hr_flow[:, 0:t:3, ...]  # e.g., frame1 -> frame0\n",
    "                hr_flow_idle = torch.zeros_like(hr_flow_bw)\n",
    "                hr_flow_fw = hr_flow.flip(1)[:, 1:t:3, ...]\n",
    "            else:\n",
    "                lr_curr = lr_data[:, 1:t:3, ...]\n",
    "                lr_curr = lr_curr.reshape(n_clip, c, lr_h, lr_w)\n",
    "\n",
    "                lr_next = lr_data[:, 2:t:3, ...]\n",
    "                lr_next = lr_next.reshape(n_clip, c, lr_h, lr_w)\n",
    "\n",
    "                # compute forward flow\n",
    "                lr_flow_fw = net_G.fnet(lr_curr, lr_next)\n",
    "                hr_flow_fw = self.scale * self.upsample_func(lr_flow_fw)\n",
    "\n",
    "                hr_flow_bw = hr_flow[:, 0:t:3, ...]  # e.g., frame1 -> frame0\n",
    "                hr_flow_idle = torch.zeros_like(hr_flow_bw)  # frame1 -> frame1\n",
    "                hr_flow_fw = hr_flow_fw.view(n, t // 3, 2, hr_h, hr_w)  # frame1 -> frame2\n",
    "\n",
    "            # merge bw/idle/fw flows\n",
    "            hr_flow_merge = torch.stack(\n",
    "                [hr_flow_bw, hr_flow_idle, hr_flow_fw], dim=2)  # n,t//3,3,2,h,w\n",
    "\n",
    "            # reshape and stop gradient propagation\n",
    "            hr_flow_merge = hr_flow_merge.view(n_clip * 3, 2, hr_h, hr_w).detach()\n",
    "\n",
    "        else:\n",
    "            # reused data to reduce computations\n",
    "            hr_flow_merge = args_dict['hr_flow_merge']\n",
    "\n",
    "        # === build up inputs for D (3 parts) === #\n",
    "        # part 1: bicubic upsampled data (conditional inputs)\n",
    "        cond_data = bi_data[:, :t, ...].reshape(n_clip, 3, c, hr_h, hr_w)\n",
    "        # note: permutation is not necessarily needed here, it's just to keep\n",
    "        #       the same impl. as TecoGAN-Tensorflow (i.e., rrrgggbbb)\n",
    "        cond_data = cond_data.permute(0, 2, 1, 3, 4)\n",
    "        cond_data = cond_data.reshape(n_clip, c * 3, hr_h, hr_w)\n",
    "\n",
    "        # part 2: original data\n",
    "        orig_data = data[:, :t, ...].reshape(n_clip, 3, c, hr_h, hr_w)\n",
    "        orig_data = orig_data.permute(0, 2, 1, 3, 4)\n",
    "        orig_data = orig_data.reshape(n_clip, c * 3, hr_h, hr_w)\n",
    "\n",
    "        # part 3: warped data\n",
    "        warp_data = backward_warp(\n",
    "            data[:, :t, ...].reshape(n * t, c, hr_h, hr_w), hr_flow_merge)\n",
    "        warp_data = warp_data.view(n_clip, 3, c, hr_h, hr_w)\n",
    "        warp_data = warp_data.permute(0, 2, 1, 3, 4)\n",
    "        warp_data = warp_data.reshape(n_clip, c * 3, hr_h, hr_w)\n",
    "        # remove border to increase training stability as proposed in TecoGAN\n",
    "        warp_data = F.pad(\n",
    "            warp_data[..., n_pad: n_pad + c_size, n_pad: n_pad + c_size],\n",
    "            (n_pad,) * 4, mode='constant')\n",
    "\n",
    "        # combine 3 parts together\n",
    "        input_data = torch.cat([orig_data, warp_data, cond_data], dim=1)\n",
    "\n",
    "        # === classify === #\n",
    "        out = self.conv_in(input_data)\n",
    "        out, feature_list = self.discriminator_block(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dense(out)\n",
    "        pred = out, feature_list\n",
    "\n",
    "        # construct output dict (return other data beside pred)\n",
    "        ret_dict = {\n",
    "            'hr_flow_merge': hr_flow_merge\n",
    "        }\n",
    "\n",
    "        return pred, ret_dict\n",
    "\n",
    "\n",
    "class SpatialDiscriminator(BaseSequenceDiscriminator):\n",
    "    \"\"\" Spatial discriminator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_nc, spatial_size, use_cond):\n",
    "        super(SpatialDiscriminator, self).__init__()\n",
    "\n",
    "        # basic settings\n",
    "        self.use_cond = use_cond  # whether to use conditional input\n",
    "        mult = 2 if self.use_cond else 1\n",
    "        tempo_range = 1\n",
    "\n",
    "        # input conv\n",
    "        self.conv_in = nn.Sequential(\n",
    "            nn.Conv2d(in_nc*tempo_range*mult, 64, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        # discriminator block\n",
    "        self.discriminator_block = DiscriminatorBlocks()  # /16\n",
    "\n",
    "        # classifier\n",
    "        self.dense = nn.Linear(256 * spatial_size // 16 * spatial_size // 16, 1)\n",
    "\n",
    "    def forward(self, data, args_dict):\n",
    "        out = self.forward_sequence(data, args_dict)\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        out = self.conv_in(x)\n",
    "        out, feature_list = self.discriminator_block(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dense(out)\n",
    "\n",
    "        return out, feature_list\n",
    "\n",
    "    def forward_sequence(self, data, args_dict):\n",
    "        # === set params === #\n",
    "        n, t, c, hr_h, hr_w = data.size()\n",
    "        data = data.view(n * t, c, hr_h, hr_w)\n",
    "\n",
    "        # === build up inputs for net_D === #\n",
    "        if self.use_cond:\n",
    "            bi_data = args_dict['bi_data'].view(n * t, c, hr_h, hr_w)\n",
    "            input_data = torch.cat([bi_data, data], dim=1)\n",
    "        else:\n",
    "            input_data = data\n",
    "\n",
    "        # === classify === #\n",
    "        pred = self.step(input_data)\n",
    "\n",
    "        # construct output dict (nothing to return)\n",
    "        ret_dict = {}\n",
    "\n",
    "        return pred, ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def define_generator(opt):\n",
    "    net_G_opt = opt['model']['generator']\n",
    "\n",
    "    if net_G_opt['name'].lower() == 'frnet':  # frame-recurrent generator\n",
    "        net_G = FRNet(\n",
    "            in_nc=net_G_opt['in_nc'],\n",
    "            out_nc=net_G_opt['out_nc'],\n",
    "            nf=net_G_opt['nf'],\n",
    "            nb=net_G_opt['nb'],\n",
    "            degradation=opt['dataset']['degradation']['type'],\n",
    "            scale=opt['scale'])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Unrecognized generator: {net_G_opt[\"name\"]}')\n",
    "\n",
    "    return net_G\n",
    "\n",
    "\n",
    "def define_discriminator(opt):\n",
    "    net_D_opt = opt['model']['discriminator']\n",
    "\n",
    "    if opt['dataset']['degradation']['type'] == 'BD':\n",
    "        spatial_size = opt['dataset']['train']['crop_size']\n",
    "    else:  # BI\n",
    "        spatial_size = opt['dataset']['train']['gt_crop_size']\n",
    "\n",
    "    if net_D_opt['name'].lower() == 'stnet':  # spatio-temporal discriminator\n",
    "        net_D = SpatioTemporalDiscriminator(\n",
    "            in_nc=net_D_opt['in_nc'],\n",
    "            spatial_size=spatial_size,\n",
    "            tempo_range=net_D_opt['tempo_range'],\n",
    "            degradation=opt['dataset']['degradation']['type'],\n",
    "            scale=opt['scale'])\n",
    "\n",
    "    elif net_D_opt['name'].lower() == 'snet':  # spatial discriminator\n",
    "        net_D = SpatialDiscriminator(\n",
    "            in_nc=net_D_opt['in_nc'],\n",
    "            spatial_size=spatial_size,\n",
    "            use_cond=net_D_opt['use_cond'])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Unrecognized discriminator: {net_D_opt[\"name\"]}')\n",
    "\n",
    "    return net_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Code adopted from BasicSR: https://github.com/xinntao/BasicSR/blob/master/basicsr/models/lr_scheduler.py\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "def get_position_from_periods(iteration, cumulative_period):\n",
    "    \"\"\"Get the position from a period list.\n",
    "\n",
    "    It will return the index of the right-closest number in the period list.\n",
    "    For example, the cumulative_period = [100, 200, 300, 400],\n",
    "    if iteration == 50, return 0;\n",
    "    if iteration == 210, return 2;\n",
    "    if iteration == 300, return 2.\n",
    "\n",
    "    Args:\n",
    "        iteration (int): Current iteration.\n",
    "        cumulative_period (list[int]): Cumulative period list.\n",
    "\n",
    "    Returns:\n",
    "        int: The position of the right-closest number in the period list.\n",
    "    \"\"\"\n",
    "    for i, period in enumerate(cumulative_period):\n",
    "        if iteration <= period:\n",
    "            return i\n",
    "\n",
    "\n",
    "class CosineAnnealingRestartLR(_LRScheduler):\n",
    "    \"\"\" Cosine annealing with restarts learning rate scheme.\n",
    "\n",
    "    An example of config:\n",
    "    periods = [10, 10, 10, 10]\n",
    "    restart_weights = [1, 0.5, 0.5, 0.5]\n",
    "    eta_min=1e-7\n",
    "\n",
    "    It has four cycles, each has 10 iterations. At 10th, 20th, 30th, the\n",
    "    scheduler will restart with the weights in restart_weights.\n",
    "\n",
    "    Args:\n",
    "        optimizer (torch.nn.optimizer): Torch optimizer.\n",
    "        periods (list): Period for each cosine anneling cycle.\n",
    "        restart_weights (list): Restart weights at each restart iteration.\n",
    "            Default: [1].\n",
    "        eta_min (float): The mimimum lr. Default: 0.\n",
    "        last_epoch (int): Used in _LRScheduler. Default: -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer,\n",
    "                 periods,\n",
    "                 restart_weights=(1, ),\n",
    "                 eta_min=0,\n",
    "                 last_epoch=-1):\n",
    "        self.periods = periods\n",
    "        self.restart_weights = restart_weights\n",
    "        self.eta_min = eta_min\n",
    "        assert (len(self.periods) == len(self.restart_weights)\n",
    "                ), 'periods and restart_weights should have the same length.'\n",
    "        self.cumulative_period = [\n",
    "            sum(self.periods[0:i + 1]) for i in range(0, len(self.periods))\n",
    "        ]\n",
    "        super(CosineAnnealingRestartLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        idx = get_position_from_periods(self.last_epoch,\n",
    "                                        self.cumulative_period)\n",
    "        current_weight = self.restart_weights[idx]\n",
    "        nearest_restart = 0 if idx == 0 else self.cumulative_period[idx - 1]\n",
    "        current_period = self.periods[idx]\n",
    "\n",
    "        return [\n",
    "            self.eta_min + current_weight * 0.5 * (base_lr - self.eta_min) *\n",
    "            (1 + math.cos(math.pi * (\n",
    "                (self.last_epoch - nearest_restart) / current_period)))\n",
    "            for base_lr in self.base_lrs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VanillaGANLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(VanillaGANLoss, self).__init__()\n",
    "        self.crit = nn.BCEWithLogitsLoss(reduction=reduction)\n",
    "\n",
    "    def forward(self, input, status):\n",
    "        target = torch.empty_like(input).fill_(int(status))\n",
    "        loss = self.crit(input, target)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class LSGANLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(LSGANLoss, self).__init__()\n",
    "        self.crit = nn.MSELoss(reduction=reduction)\n",
    "\n",
    "    def forward(self, input, status):\n",
    "        \"\"\"\n",
    "            :param status: boolean, True/False\n",
    "        \"\"\"\n",
    "        target = torch.empty_like(input).fill_(int(status))\n",
    "        loss = self.crit(input, target)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class CharbonnierLoss(nn.Module):\n",
    "    \"\"\" Charbonnier Loss (robust L1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eps=1e-6, reduction='sum'):\n",
    "        super(CharbonnierLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        diff = x - y\n",
    "        loss = torch.sqrt(diff * diff + self.eps)\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = torch.mean(loss)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return loss\n",
    "\n",
    "\n",
    "class CosineSimilarityLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super(CosineSimilarityLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        diff = F.cosine_similarity(input, target, dim=1, eps=self.eps)\n",
    "        loss = 1.0 - diff.mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "def define_criterion(criterion_opt):\n",
    "    if criterion_opt is None:\n",
    "        return None\n",
    "\n",
    "    # parse\n",
    "    if criterion_opt['type'] == 'MSE':\n",
    "        criterion = nn.MSELoss(reduction=criterion_opt['reduction'])\n",
    "\n",
    "    elif criterion_opt['type'] == 'L1':\n",
    "        criterion = nn.L1Loss(reduction=criterion_opt['reduction'])\n",
    "\n",
    "    elif criterion_opt['type'] == 'CB':\n",
    "        criterion = CharbonnierLoss(reduction=criterion_opt['reduction'])\n",
    "\n",
    "    elif criterion_opt['type'] == 'CosineSimilarity':\n",
    "        criterion = CosineSimilarityLoss()\n",
    "\n",
    "    elif criterion_opt['type'] == 'GAN':\n",
    "        criterion = VanillaGANLoss(reduction=criterion_opt['reduction'])\n",
    "\n",
    "    elif criterion_opt['type'] == 'LSGAN':\n",
    "        criterion = LSGANLoss(reduction=criterion_opt['reduction'])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Unrecognized criterion: {criterion_opt[\"type\"]}')\n",
    "\n",
    "    return criterion\n",
    "\n",
    "\n",
    "def define_lr_schedule(schedule_opt, optimizer):\n",
    "    if schedule_opt is None:\n",
    "        return None\n",
    "\n",
    "    # parse\n",
    "    if schedule_opt['type'] == 'FixedLR':\n",
    "        schedule = None\n",
    "\n",
    "    elif schedule_opt['type'] == 'MultiStepLR':\n",
    "        schedule = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer,\n",
    "            milestones=schedule_opt['milestones'],\n",
    "            gamma=schedule_opt['gamma'])\n",
    "\n",
    "    elif schedule_opt['type'] == 'CosineAnnealingRestartLR':\n",
    "        schedule = CosineAnnealingRestartLR(\n",
    "            optimizer,\n",
    "            periods=schedule_opt['periods'],\n",
    "            restart_weights=schedule_opt['restart_weights'],\n",
    "            eta_min=schedule_opt['eta_min'])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Unrecognized lr schedule: {schedule_opt[\"type\"]}')\n",
    "\n",
    "    return schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VSRModel(BaseModel):\n",
    "    \"\"\" A model wrapper for objective video super-resolution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        super(VSRModel, self).__init__(opt)\n",
    "\n",
    "        # define network\n",
    "        self.set_networks()\n",
    "\n",
    "        # config training\n",
    "        if self.is_train:\n",
    "            self.set_criterions()\n",
    "            self.set_optimizers()\n",
    "            self.set_lr_schedules()\n",
    "\n",
    "    def set_networks(self):\n",
    "        # define generator\n",
    "        self.net_G = define_generator(self.opt)\n",
    "        self.net_G = self.model_to_device(self.net_G)\n",
    "        \n",
    "        # load generator\n",
    "        load_path_G = self.opt['model']['generator'].get('load_path')\n",
    "        if load_path_G is not None:\n",
    "            self.load_network(self.net_G, load_path_G)\n",
    "\n",
    "    def set_criterions(self):\n",
    "        # pixel criterion\n",
    "        self.pix_crit = define_criterion(\n",
    "            self.opt['train'].get('pixel_crit'))\n",
    "\n",
    "        # warping criterion\n",
    "        self.warp_crit = define_criterion(\n",
    "            self.opt['train'].get('warping_crit'))\n",
    "\n",
    "    def set_optimizers(self):\n",
    "        self.optim_G = torch.optim.Adam(\n",
    "            self.net_G.parameters(),\n",
    "            lr=self.opt['train']['generator']['lr'],\n",
    "            weight_decay=self.opt['train']['generator'].get('weight_decay', 0),\n",
    "            betas=self.opt['train']['generator'].get('betas', (0.9, 0.999)))\n",
    "\n",
    "    def set_lr_schedules(self):\n",
    "        self.sched_G = define_lr_schedule(\n",
    "            self.opt['train']['generator'].get('lr_schedule'), self.optim_G)\n",
    "\n",
    "    def train(self):\n",
    "        # === initialize === #\n",
    "        self.net_G.train()\n",
    "        self.optim_G.zero_grad()\n",
    "\n",
    "        # === forward net_G === #\n",
    "        net_G_output_dict = self.net_G(self.lr_data)\n",
    "        self.hr_data = net_G_output_dict['hr_data']\n",
    "\n",
    "        # === optimize net_G === #\n",
    "        loss_G = 0\n",
    "        self.log_dict = OrderedDict()\n",
    "\n",
    "        # pixel loss\n",
    "        pix_w = self.opt['train']['pixel_crit'].get('weight', 1.0)\n",
    "        loss_pix_G = pix_w * self.pix_crit(self.hr_data, self.gt_data)\n",
    "        loss_G += loss_pix_G\n",
    "        self.log_dict['l_pix_G'] = loss_pix_G.item()\n",
    "\n",
    "        # warping loss\n",
    "        if self.warp_crit is not None:\n",
    "            # warp lr_prev according to lr_flow\n",
    "            lr_curr = net_G_output_dict['lr_curr']\n",
    "            lr_prev = net_G_output_dict['lr_prev']\n",
    "            lr_flow = net_G_output_dict['lr_flow']\n",
    "            lr_warp = backward_warp(lr_prev, lr_flow)\n",
    "\n",
    "            warp_w = self.opt['train']['warping_crit'].get('weight', 1.0)\n",
    "            loss_warp_G = warp_w * self.warp_crit(lr_warp, lr_curr)\n",
    "            loss_G += loss_warp_G\n",
    "            self.log_dict['l_warp_G'] = loss_warp_G.item()\n",
    "\n",
    "        # optimize\n",
    "        loss_G.backward()\n",
    "        self.optim_G.step()\n",
    "\n",
    "    def infer(self):\n",
    "        \"\"\" Infer the `lr_data` sequence\n",
    "\n",
    "            :return: np.ndarray sequence in type [uint8] and shape [thwc]\n",
    "        \"\"\"\n",
    "\n",
    "        lr_data = self.lr_data\n",
    "\n",
    "        # temporal padding\n",
    "        lr_data, n_pad_front = self.pad_sequence(lr_data)\n",
    "\n",
    "        # infer\n",
    "        self.net_G.eval()\n",
    "        hr_seq = self.net_G(lr_data, self.device)\n",
    "        hr_seq = hr_seq[n_pad_front:]\n",
    "\n",
    "        return hr_seq\n",
    "\n",
    "    def save(self, current_iter):\n",
    "        self.save_network(self.net_G, 'G', current_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSRGANModel(VSRModel):\n",
    "    \"\"\" A model wrapper for subjective video super-resolution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        super(VSRGANModel, self).__init__(opt)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.cnt_upd_D = 0\n",
    "\n",
    "    def set_networks(self):\n",
    "        # define generator\n",
    "        self.net_G = define_generator(self.opt)\n",
    "        self.net_G = self.model_to_device(self.net_G)\n",
    "        \n",
    "        # load generator\n",
    "        load_path_G = self.opt['model']['generator'].get('load_path', '')\n",
    "        if load_path_G:\n",
    "            self.load_network(self.net_G, load_path_G)\n",
    "        \n",
    "        if self.is_train:\n",
    "            # define discriminator\n",
    "            self.net_D = define_discriminator(self.opt)\n",
    "            self.net_D = self.model_to_device(self.net_D)\n",
    "            \n",
    "            # load discriminator\n",
    "            load_path_D = self.opt['model']['discriminator'].get('load_path', '')\n",
    "            if load_path_D:\n",
    "                self.load_network(self.net_D, load_path_D)\n",
    "\n",
    "    def set_criterions(self):\n",
    "        # pixel criterion\n",
    "        self.pix_crit = define_criterion(self.opt['train'].get('pixel_crit'))\n",
    "\n",
    "        # warping criterion\n",
    "        self.warp_crit = define_criterion(self.opt['train'].get('warping_crit'))\n",
    "\n",
    "        # feature criterion\n",
    "        self.feat_crit = define_criterion(\n",
    "            self.opt['train'].get('feature_crit'))\n",
    "        if self.feat_crit is not None:  # load feature extractor\n",
    "            feature_layers = self.opt['train']['feature_crit'].get(\n",
    "                'feature_layers', [8, 17, 26, 35])\n",
    "            self.net_F = VGGFeatureExtractor(feature_layers).to(self.device)\n",
    "\n",
    "        # ping-pong criterion\n",
    "        self.pp_crit = define_criterion(self.opt['train'].get('pingpong_crit'))\n",
    "\n",
    "        # feature matching criterion\n",
    "        self.fm_crit = define_criterion(self.opt['train'].get('feature_matching_crit'))\n",
    "\n",
    "        # gan criterion\n",
    "        self.gan_crit = define_criterion(self.opt['train'].get('gan_crit'))\n",
    "\n",
    "    def set_optimizers(self):\n",
    "        # set optimizer for net_G\n",
    "        self.optim_G = torch.optim.Adam(\n",
    "            self.net_G.parameters(),\n",
    "            lr=self.opt['train']['generator']['lr'],\n",
    "            weight_decay=self.opt['train']['generator'].get('weight_decay', 0),\n",
    "            betas=self.opt['train']['generator'].get('betas', (0.9, 0.999)))\n",
    "\n",
    "        # set optimizer for net_D\n",
    "        self.optim_D = torch.optim.Adam(\n",
    "            self.net_D.parameters(),\n",
    "            lr=self.opt['train']['discriminator']['lr'],\n",
    "            weight_decay=self.opt['train']['discriminator'].get('weight_decay', 0),\n",
    "            betas=self.opt['train']['discriminator'].get('betas', (0.9, 0.999)))\n",
    "\n",
    "    def set_lr_schedules(self):\n",
    "        # set lr schedules for net_G\n",
    "        self.sched_G = define_lr_schedule(\n",
    "            self.opt['train']['generator'].get('lr_schedule'), self.optim_G)\n",
    "\n",
    "        # set lr schedules for net_D\n",
    "        self.sched_D = define_lr_schedule(\n",
    "            self.opt['train']['discriminator'].get('lr_schedule'), self.optim_D)\n",
    "\n",
    "    def train(self):\n",
    "        # === prepare data === #\n",
    "        lr_data, gt_data = self.lr_data, self.gt_data\n",
    "\n",
    "        n, t, c, lr_h, lr_w = lr_data.size()\n",
    "        _, _, _, gt_h, gt_w = gt_data.size()\n",
    "\n",
    "        # generate bicubic upsampled data\n",
    "        upsample_fn = self.get_bare_model(self.net_G).upsample_func\n",
    "        bi_data = upsample_fn(\n",
    "            lr_data.view(n * t, c, lr_h, lr_w)).view(n, t, c, gt_h, gt_w)\n",
    "\n",
    "        # augment data for pingpong criterion\n",
    "        # i.e., (0,1,2,...,t-2,t-1) -> (0,1,2,...,t-2,t-1,t-2,...,2,1,0)\n",
    "        if self.pp_crit is not None:\n",
    "            lr_rev = lr_data.flip(1)[:, 1:, ...]\n",
    "            gt_rev = gt_data.flip(1)[:, 1:, ...]\n",
    "            bi_rev = bi_data.flip(1)[:, 1:, ...]\n",
    "\n",
    "            lr_data = torch.cat([lr_data, lr_rev], dim=1)\n",
    "            gt_data = torch.cat([gt_data, gt_rev], dim=1)\n",
    "            bi_data = torch.cat([bi_data, bi_rev], dim=1)\n",
    "\n",
    "        # === initialize === #\n",
    "        self.net_G.train()\n",
    "        self.net_D.train()\n",
    "        self.optim_G.zero_grad()\n",
    "        self.optim_D.zero_grad()\n",
    "        self.log_dict = OrderedDict()\n",
    "\n",
    "        # === forward net_G === #\n",
    "        net_G_output_dict = self.net_G(lr_data)\n",
    "        hr_data = net_G_output_dict['hr_data']\n",
    "\n",
    "        # === forward net_D === #\n",
    "        for param in self.net_D.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # feed additional data\n",
    "        net_D_input_dict = {\n",
    "            'net_G': self.get_bare_model(self.net_G),  # TODO: check\n",
    "            'lr_data': lr_data,\n",
    "            'bi_data': bi_data,\n",
    "            'use_pp_crit': (self.pp_crit is not None),\n",
    "            'crop_border_ratio': self.opt['train']['discriminator'].get(\n",
    "                'crop_border_ratio', 1.0)\n",
    "        }\n",
    "        net_D_input_dict.update(net_G_output_dict)\n",
    "\n",
    "        # forward real sequence (gt)\n",
    "        real_pred, net_D_oputput_dict = self.net_D(gt_data, net_D_input_dict)\n",
    "\n",
    "        # reuse internal data (e.g., optical flow)\n",
    "        net_D_input_dict.update(net_D_oputput_dict)\n",
    "\n",
    "        # forward fake sequence (hr)\n",
    "        fake_pred, _ = self.net_D(hr_data.detach(), net_D_input_dict)\n",
    "\n",
    "        # === optimize net_D === #\n",
    "        real_pred_D, fake_pred_D = real_pred[0], fake_pred[0]\n",
    "\n",
    "        # select D update policy\n",
    "        update_policy = self.opt['train']['discriminator']['update_policy']\n",
    "        if update_policy == 'adaptive':\n",
    "            # update D adaptively\n",
    "            logged_real_pred_D = torch.log(torch.sigmoid(real_pred_D) + 1e-8).mean()\n",
    "            logged_fake_pred_D = torch.log(torch.sigmoid(fake_pred_D) + 1e-8).mean()\n",
    "\n",
    "            if self.dist:\n",
    "                # synchronize\n",
    "                dist.all_reduce(logged_real_pred_D)\n",
    "                dist.all_reduce(logged_fake_pred_D)\n",
    "                dist.barrier()\n",
    "\n",
    "                logged_real_pred_D /= self.opt['world_size']\n",
    "                logged_fake_pred_D /= self.opt['world_size']\n",
    "\n",
    "            distance = (logged_real_pred_D - logged_fake_pred_D).item()\n",
    "            upd_D = distance < self.opt['train']['discriminator']['update_threshold']\n",
    "        else:\n",
    "            upd_D = True\n",
    "\n",
    "        if upd_D:\n",
    "            self.cnt_upd_D += 1.0\n",
    "            real_loss_D = self.gan_crit(real_pred_D, True)\n",
    "            fake_loss_D = self.gan_crit(fake_pred_D, False)\n",
    "            loss_D = real_loss_D + fake_loss_D\n",
    "\n",
    "            # update net_D\n",
    "            loss_D.backward()\n",
    "            self.optim_D.step()\n",
    "        else:\n",
    "            loss_D = torch.zeros(1)\n",
    "\n",
    "        # logging\n",
    "        self.log_dict['l_gan_D'] = loss_D.item()\n",
    "        self.log_dict['p_real_D'] = real_pred_D.mean().item()\n",
    "        self.log_dict['p_fake_D'] = fake_pred_D.mean().item()\n",
    "        if update_policy == 'adaptive':\n",
    "            self.log_dict['distance'] = distance\n",
    "            self.log_dict['n_upd_D'] = self.cnt_upd_D\n",
    "\n",
    "        # === optimize net_G === #\n",
    "        for param in self.net_D.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # calculate losses\n",
    "        loss_G = 0\n",
    "\n",
    "        # pixel (pix) loss\n",
    "        if self.pix_crit is not None:\n",
    "            pix_w = self.opt['train']['pixel_crit'].get('weight', 1)\n",
    "            loss_pix_G = pix_w * self.pix_crit(hr_data, gt_data)\n",
    "            loss_G += loss_pix_G\n",
    "            self.log_dict['l_pix_G'] = loss_pix_G.item()\n",
    "\n",
    "        # warping (warp) loss\n",
    "        if self.warp_crit is not None:\n",
    "            lr_curr = net_G_output_dict['lr_curr']\n",
    "            lr_prev = net_G_output_dict['lr_prev']\n",
    "            lr_flow = net_G_output_dict['lr_flow']\n",
    "            lr_warp = backward_warp(lr_prev, lr_flow)\n",
    "\n",
    "            warp_w = self.opt['train']['warping_crit'].get('weight', 1)\n",
    "            loss_warp_G = warp_w * self.warp_crit(lr_warp, lr_curr)\n",
    "            loss_G += loss_warp_G\n",
    "            self.log_dict['l_warp_G'] = loss_warp_G.item()\n",
    "\n",
    "        # feature/perceptual (feat) loss\n",
    "        if self.feat_crit is not None:\n",
    "            hr_merge = hr_data.view(-1, c, gt_h, gt_w)\n",
    "            gt_merge = gt_data.view(-1, c, gt_h, gt_w)\n",
    "\n",
    "            hr_feat_lst = self.net_F(hr_merge)\n",
    "            gt_feat_lst = self.net_F(gt_merge)\n",
    "\n",
    "            loss_feat_G = 0\n",
    "            for hr_feat, gt_feat in zip(hr_feat_lst, gt_feat_lst):\n",
    "                loss_feat_G += self.feat_crit(hr_feat, gt_feat.detach())\n",
    "\n",
    "            feat_w = self.opt['train']['feature_crit'].get('weight', 1)\n",
    "            loss_feat_G = feat_w * loss_feat_G\n",
    "            loss_G += loss_feat_G\n",
    "            self.log_dict['l_feat_G'] = loss_feat_G.item()\n",
    "\n",
    "        # ping-pong (pp) loss\n",
    "        if self.pp_crit is not None:\n",
    "            tempo_extent = self.opt['train']['tempo_extent']\n",
    "            hr_data_fw = hr_data[:, :tempo_extent - 1, ...]      # -------->|\n",
    "            hr_data_bw = hr_data[:, tempo_extent:, ...].flip(1)  # <--------|\n",
    "\n",
    "            pp_w = self.opt['train']['pingpong_crit'].get('weight', 1)\n",
    "            loss_pp_G = pp_w * self.pp_crit(hr_data_fw, hr_data_bw)\n",
    "            loss_G += loss_pp_G\n",
    "            self.log_dict['l_pp_G'] = loss_pp_G.item()\n",
    "\n",
    "        # feature matching (fm) loss\n",
    "        if self.fm_crit is not None:\n",
    "            fake_pred, _ = self.net_D(hr_data, net_D_input_dict)\n",
    "            fake_feat_lst, real_feat_lst = fake_pred[-1], real_pred[-1]\n",
    "\n",
    "            layer_norm = self.opt['train']['feature_matching_crit'].get(\n",
    "                'layer_norm', [12.0, 14.0, 24.0, 100.0])\n",
    "\n",
    "            loss_fm_G = 0\n",
    "            for i in range(len(real_feat_lst)):\n",
    "                fake_feat, real_feat = fake_feat_lst[i], real_feat_lst[i]\n",
    "                loss_fm_G += self.fm_crit(\n",
    "                    fake_feat, real_feat.detach()) / layer_norm[i]\n",
    "\n",
    "            fm_w = self.opt['train']['feature_matching_crit'].get('weight', 1)\n",
    "            loss_fm_G = fm_w * loss_fm_G\n",
    "            loss_G += loss_fm_G\n",
    "            self.log_dict['l_fm_G'] = loss_fm_G.item()\n",
    "\n",
    "        # gan loss\n",
    "        if self.fm_crit is None:\n",
    "            fake_pred, _ = self.net_D(hr_data, net_D_input_dict)\n",
    "        fake_pred_G = fake_pred[0]\n",
    "\n",
    "        gan_w = self.opt['train']['gan_crit'].get('weight', 1)\n",
    "        loss_gan_G = gan_w * self.gan_crit(fake_pred_G, True)\n",
    "        loss_G += loss_gan_G\n",
    "        self.log_dict['l_gan_G'] = loss_gan_G.item()\n",
    "        self.log_dict['p_fake_G'] = fake_pred_G.mean(W).item()\n",
    "\n",
    "        # update net_G\n",
    "        loss_G.backward()\n",
    "        self.optim_G.step()\n",
    "\n",
    "    def save(self, current_iter):\n",
    "        self.save_network(self.net_G, 'G', current_iter)\n",
    "        self.save_network(self.net_D, 'D', current_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ColorUNet(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 153\u001b[0m, in \u001b[0;36mColorUNet.forward\u001b[0;34m(self, L, prev_ab)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, L, prev_ab):\n\u001b[1;32m    152\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([L, prev_ab], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/fastai/layers.py:409\u001b[0m, in \u001b[0;36mSequentialEx.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    408\u001b[0m     res\u001b[38;5;241m.\u001b[39morig \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 409\u001b[0m     nres \u001b[38;5;241m=\u001b[39m \u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# We have to remove res.orig to avoid hanging refs and therefore memory leaks\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     res\u001b[38;5;241m.\u001b[39morig, nres\u001b[38;5;241m.\u001b[39morig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "model(torch.rand(1,1,256,256),torch.rand(1,2,256,256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
