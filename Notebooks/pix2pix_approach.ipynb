{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import lmdb\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import kornia\n",
    "from skimage.color import lab2rgb\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "# pip install fastai==2.4\n",
    "from fastai.vision.learner import create_body\n",
    "from torchvision.models.resnet import resnet18\n",
    "from fastai.vision.models.unet import DynamicUnet\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "TEMPO_LENGTH = 4 # (only used while trianing)\n",
    "SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_to_rgb(L, ab):\n",
    "    \"\"\"\n",
    "    Takes a batch of images\n",
    "    \"\"\"\n",
    "\n",
    "    L = (L + 1.) * 50.\n",
    "    ab = ab * 110.\n",
    "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
    "    rgb_imgs = []\n",
    "    for img in Lab:\n",
    "        img_rgb = lab2rgb(img)\n",
    "        rgb_imgs.append(img_rgb)\n",
    "    return np.stack(rgb_imgs, axis=0)\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data_opt, **kwargs):\n",
    "        # dict to attr\n",
    "        for kw, args in data_opt.__dict__.items():\n",
    "            setattr(self, kw, args)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        pass\n",
    "\n",
    "   \n",
    "    @staticmethod\n",
    "    def init_lmdb(seq_dir):\n",
    "        env = lmdb.open(\n",
    "            seq_dir, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        return env\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_lmdb_key(key):\n",
    "        key_lst = key.split('_')\n",
    "        idx, size, frm = key_lst[:-2], key_lst[-2], int(key_lst[-1])\n",
    "        idx = '_'.join(idx)\n",
    "        size = tuple(map(int, size.split('x')))  # n_frm, h, w\n",
    "        return idx, size, frm\n",
    "\n",
    "    @staticmethod\n",
    "    def read_lmdb_frame(env, key, size):\n",
    "        with env.begin(write=False) as txn:\n",
    "            buf = txn.get(key.encode('ascii'))\n",
    "        frm = np.frombuffer(buf, dtype=np.uint8).reshape(*size)\n",
    "        return frm\n",
    "\n",
    "    def crop_sequence(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def augment_sequence(**kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class PairedLMDBDataset(BaseDataset):\n",
    "    \"\"\" LMDB dataset for paired data (for BI degradation)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_opt, **kwargs):\n",
    "        super(PairedLMDBDataset, self).__init__(data_opt, **kwargs)\n",
    "        \n",
    "        self.lr_seq_dir = data_opt.lr_seq_dir\n",
    "        self.data_type = data_opt.data_type\n",
    "        self.scale = data_opt.scale\n",
    "        self.tempo_extent = data_opt.tempo_extent\n",
    "        self.moving_first_frame = data_opt.moving_first_frame\n",
    "        self.moving_factor = data_opt.moving_factor\n",
    "        self.filter_file = data_opt.filter_file\n",
    "        \n",
    "        self.transform = torchvision.transforms.Compose([torchvision.transforms.Resize((256, 256))])\n",
    "\n",
    "        # load meta info\n",
    "        lr_meta = pickle.load(\n",
    "            open(osp.join(self.lr_seq_dir, 'meta_info.pkl'), 'rb'))\n",
    "        self.lr_keys = sorted(lr_meta['keys'])\n",
    "        \n",
    "        # register parameters\n",
    "        self.lr_env = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_keys)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.lr_env is None:\n",
    "            self.lr_env = self.init_lmdb(self.lr_seq_dir)\n",
    "\n",
    "        # parse info\n",
    "        lr_key = self.lr_keys[item]\n",
    "        idx, (tot_frm, lr_h, lr_w), cur_frm = self.parse_lmdb_key(lr_key)\n",
    "\n",
    "        c = 3 if self.data_type.lower() == 'rgb' else 1\n",
    "        \n",
    "        # get frames\n",
    "        lr_frms = []\n",
    "        if self.moving_first_frame and (random.uniform(0, 1) > self.moving_factor):\n",
    "            # load the first gt&lr frame\n",
    "            lr_frm = self.read_lmdb_frame(\n",
    "                self.lr_env, lr_key, size=(lr_h, lr_w, c))\n",
    "            lr_frm = lr_frm.transpose(2, 0, 1)  # chw|rgb|uint8\n",
    "\n",
    "            # generate random moving parameters\n",
    "            offsets = np.floor(\n",
    "                np.random.uniform(-1.5, 1.5, size=(self.tempo_extent, 2)))\n",
    "            offsets = offsets.astype(np.int32)\n",
    "            pos = np.cumsum(offsets, axis=0)\n",
    "            min_pos = np.min(pos, axis=0)\n",
    "            topleft_pos = pos - min_pos\n",
    "            range_pos = np.max(pos, axis=0) - min_pos\n",
    "            c_h, c_w = lr_h - range_pos[0], lr_w - range_pos[1]\n",
    "\n",
    "            # generate frames\n",
    "            for i in range(self.tempo_extent):\n",
    "                lr_top, lr_left = topleft_pos[i]\n",
    "                lr_frms.append(lr_frm[\n",
    "                    :, lr_top: lr_top + c_h, lr_left: lr_left + c_w].copy())\n",
    "\n",
    "        else:\n",
    "            # read frames\n",
    "            for i in range(cur_frm, cur_frm + self.tempo_extent):\n",
    "                if i >= tot_frm:\n",
    "                    # reflect temporal paddding, e.g., (0,1,2) -> (0,1,2,1,0)\n",
    "                    lr_key = '{}_{}x{}x{}_{:04d}'.format(\n",
    "                        idx, tot_frm, lr_h, lr_w, 2 * tot_frm - i - 2)\n",
    "                else:\n",
    "                    lr_key = '{}_{}x{}x{}_{:04d}'.format(\n",
    "                        idx, tot_frm, lr_h, lr_w, i)\n",
    "\n",
    "                lr_frm = self.read_lmdb_frame(\n",
    "                    self.lr_env, lr_key, size=(lr_h, lr_w, c))\n",
    "                lr_frm = lr_frm.transpose(2, 0, 1)\n",
    "                lr_frms.append(lr_frm)\n",
    "\n",
    "        lr_frms = np.stack(lr_frms)\n",
    "        lr_tsr = torch.FloatTensor(np.ascontiguousarray(lr_frms)) / 255\n",
    "        lr_tsr = self.transform(lr_tsr)\n",
    "\n",
    "        lr_tsr = kornia.color.rgb_to_lab(lr_tsr)\n",
    "        L = lr_tsr[:, 0:1, :, :]\n",
    "        ab = lr_tsr[:, 1:, :, :]\n",
    "        \n",
    "        L = L/ 50. - 1. # Between -1 and 1\n",
    "        ab = ab / 110. # Between -1 and 1\n",
    "        \n",
    "        return {'L': L, 'ab': ab}\n",
    "\n",
    " \n",
    "    @staticmethod\n",
    "    def augment_sequence(gt_pats, lr_pats):\n",
    "        # flip\n",
    "        axis = random.randint(1, 3)\n",
    "        if axis > 1:\n",
    "            gt_pats = np.flip(gt_pats, axis)\n",
    "            lr_pats = np.flip(lr_pats, axis)\n",
    "\n",
    "        # rotate 90 degree\n",
    "        k = random.randint(0, 3)\n",
    "        gt_pats = np.rot90(gt_pats, k, (2, 3))\n",
    "        lr_pats = np.rot90(lr_pats, k, (2, 3))\n",
    "\n",
    "        return gt_pats, lr_pats\n",
    "    \n",
    "\n",
    "class DatasetConfig:\n",
    "    def __init__(self, \n",
    "                 lr_seq_dir,\n",
    "                 data_type='rgb',\n",
    "                 scale=1,\n",
    "                 tempo_extent=5,\n",
    "                 moving_first_frame=False,\n",
    "                 moving_factor=0.5,\n",
    "                 filter_file=None):\n",
    "        self.lr_seq_dir = lr_seq_dir\n",
    "        self.data_type = data_type\n",
    "        self.scale = scale\n",
    "        self.tempo_extent = tempo_extent\n",
    "        self.moving_first_frame = moving_first_frame\n",
    "        self.moving_factor = moving_factor\n",
    "        self.filter_file = filter_file\n",
    "\n",
    "data_opt = DatasetConfig(\n",
    "    lr_seq_dir='/media/moose/Moose/Dataset/AMD/',\n",
    "    data_type='rgb',\n",
    "    scale=1,\n",
    "    tempo_extent=TEMPO_LENGTH,\n",
    "    moving_first_frame=False,\n",
    "    moving_factor=0.5,\n",
    "    filter_file=None\n",
    ")\n",
    "\n",
    "ds = PairedLMDBDataset(data_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
    "        if gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "\n",
    "    def get_labels(self, preds, target_is_real):\n",
    "        if target_is_real:\n",
    "            labels = self.real_label\n",
    "        else:\n",
    "            labels = self.fake_label\n",
    "        return labels.expand_as(preds)\n",
    "\n",
    "    def __call__(self, preds, target_is_real):\n",
    "        labels = self.get_labels(preds, target_is_real)\n",
    "        loss = self.loss(preds, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init='norm', gain=0.02):\n",
    "\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
    "            if init == 'norm':\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
    "            elif init == 'xavier':\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init == 'kaiming':\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif 'BatchNorm2d' in classname:\n",
    "            nn.init.normal_(m.weight.data, 1., gain)\n",
    "            nn.init.constant_(m.bias.data, 0.)\n",
    "\n",
    "    net.apply(init_func)\n",
    "    print(f\"model initialized with {init} initialization\")\n",
    "    return net\n",
    "\n",
    "def init_model(model, device):\n",
    "    model = model.to(device)\n",
    "    model = init_weights(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_c, num_filters=64, n_down=3):\n",
    "        super().__init__()\n",
    "        model = [self.get_layers(input_c, num_filters, norm=False)]\n",
    "        model += [self.get_layers(num_filters * 2 ** i, num_filters * 2 ** (i + 1), s=1 if i == (n_down-1) else 2)\n",
    "                          for i in range(n_down)] # the 'if' statement is taking care of not using\n",
    "                                                  # stride of 2 for the last block in this loop\n",
    "        model += [self.get_layers(num_filters * 2 ** n_down, 1, s=1, norm=False, act=False)] # Make sure to not use normalization or\n",
    "                                                                                             # activation for the last layer of the model\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True): # when needing to make some repeatitive blocks of layers,\n",
    "        layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]          # it's always helpful to make a separate method for that purpose\n",
    "        if norm: layers += [nn.BatchNorm2d(nf)]\n",
    "        if act: layers += [nn.LeakyReLU(0.2, True)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_res_unet(n_input=1, n_output=2, size=SIZE):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    body = create_body(resnet18(), pretrained=True, n_in=n_input, cut=-2)\n",
    "    net_G = DynamicUnet(body, n_output, (size, size)).to(device)\n",
    "    return net_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDIM_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.percent_ltr_input=nn.Parameter(torch.empty(size).normal_(mean=0.0,std=1.0),requires_grad=True)\n",
    "        self.percent_ltr_stm_wt=nn.Parameter(torch.empty(size).normal_(mean=0.0,std=1.0),requires_grad=True)\n",
    "        self.b1=nn.Parameter(torch.tensor(0.),requires_grad=False)\n",
    "\n",
    "        self.percent_potential_ltm_stm_wt=nn.Parameter(torch.empty(size).normal_(mean=0.0,std=1.0),requires_grad=True)\n",
    "        self.percent_potential_ltm_input=nn.Parameter(torch.empty(size).normal_(mean=0.0,std=1.0),requires_grad=True)\n",
    "        self.b2=nn.Parameter(torch.tensor(0.),requires_grad=False)\n",
    "        \n",
    "        self.potential_ltm_stm_wt=nn.Parameter(torch.empty(size).normal_(mean=0.0,std=1.0),requires_grad=True)\n",
    "        self.potential_ltm_input=nn.Parameter(torch.empty(size).normal_(mean=0.0,std=1.0),requires_grad=True)\n",
    "        self.b3=nn.Parameter(torch.tensor(0.),requires_grad=False)\n",
    "        \n",
    "        self.output_stm_contri_stm_wt=nn.Parameter(torch.empty(size).normal_(mean=0.0,std=1.0),requires_grad=True)\n",
    "        self.output_stm_contri_input=nn.Parameter(torch.empty(size).normal_(mean=0.0,std=1.0),requires_grad=True)\n",
    "        self.b4=nn.Parameter(torch.tensor(0.),requires_grad=False)\n",
    "\n",
    "    def lstm_unit(self,input_value,long_memory,short_memory):\n",
    "        \n",
    "        long_remember_percent=torch.sigmoid((input_value*self.percent_ltr_input)+\n",
    "                                            (self.percent_ltr_stm_wt*short_memory)+\n",
    "                                            self.b1)\n",
    "        \n",
    "        potential_remember_percent=torch.sigmoid((input_value*self.percent_potential_ltm_input)+\n",
    "                                                 (short_memory*self.percent_potential_ltm_stm_wt)+\n",
    "                                                  self.b2)\n",
    "\n",
    "        potential_memory = torch.tanh((short_memory * self.potential_ltm_stm_wt) + \n",
    "                                  (input_value * self.potential_ltm_input) + \n",
    "                                  self.b3)\n",
    "        \n",
    "        updated_long_memory = ((long_memory * long_remember_percent) + \n",
    "               (potential_remember_percent * potential_memory))\n",
    "\n",
    "        output_percent = torch.sigmoid((short_memory * self.output_stm_contri_stm_wt) + \n",
    "                                       (input_value * self.output_stm_contri_input) + \n",
    "                                       self.b4)         \n",
    "        \n",
    "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
    "        \n",
    "        \n",
    "        updated_long_memory = torch.tanh(updated_long_memory)\n",
    "        updated_short_memory = torch.tanh(updated_short_memory)\n",
    "        \n",
    "        return([updated_long_memory, updated_short_memory])\n",
    "\n",
    "    def forward(self, input, long_memory=0, short_memory=0): \n",
    "        \n",
    "        return self.lstm_unit(input,long_memory,short_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.unet = build_res_unet(n_input=3, n_output=2, size=SIZE)\n",
    "        self.lstm = NDIM_LSTM((2,SIZE,SIZE)).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    \n",
    "    def forward(self, L, prev_ab = None):\n",
    "        if(prev_ab is None):\n",
    "            #print(torch.squeeze(L,dim=1).shape)\n",
    "            L = torch.squeeze(L,dim=1)\n",
    "            n,c,h,w = L.shape\n",
    "            prev_ab = torch.zeros(n, 2, h, w, device='cuda' if torch.cuda.is_available() else 'cpu', dtype=torch.float32)\n",
    "        x = torch.concat([L, prev_ab], dim=1)\n",
    "        pred_ab = self.unet(x)\n",
    "        stm, ltm = self.lstm(pred_ab)\n",
    "        return {'pred_ab': pred_ab, 'stm': stm, 'ltm': ltm}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorNetGAN(nn.Module):\n",
    "    def __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4,\n",
    "                 beta1=0.5, beta2=0.999, lambda_L1=100.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.lambda_L1 = lambda_L1\n",
    "\n",
    "        if net_G is None:\n",
    "            self.net_G = ColorNet().to(self.device)\n",
    "        else:\n",
    "            self.net_G = net_G.to(self.device)\n",
    "        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n",
    "        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n",
    "        self.L1criterion = nn.L1Loss()\n",
    "        self.opt_G = torch.optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
    "        self.opt_D = torch.optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
    "\n",
    "    def set_requires_grad(self, model, requires_grad=True):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "\n",
    "    def setup_input(self, data):\n",
    "        self.L = data['L'].to(self.device)\n",
    "        self.ab = data['ab'].to(self.device)\n",
    "\n",
    "    def forward(self):\n",
    "        n, self.t, c, lr_h, lr_w = self.L.size()\n",
    "        ab_data = []\n",
    "        \n",
    "        preds = self.net_G(torch.unsqueeze(self.L[:, 0, ...],dim=1))\n",
    "        \n",
    "        \n",
    "        ab_data.append(preds['stm'])\n",
    "\n",
    "        # compute the remaining hr data\n",
    "        for i in range(1, self.t):\n",
    "            ab_curr = self.net_G(self.L[:, i, ...], ab_data[i-1])\n",
    "            ab_data.append(ab_curr['stm'])\n",
    "\n",
    "        self.ab_data = torch.stack(ab_data, dim=1)\n",
    "\n",
    "    def backward_D(self):\n",
    "        self.loss_D_fake = 0\n",
    "        self.loss_D_real = 0\n",
    "\n",
    "        for i in range(self.t):\n",
    "            fake_image = torch.cat([self.L[:, i, ...], self.ab_data[:, i, ...]], dim=1)\n",
    "            fake_preds = self.net_D(fake_image.detach())\n",
    "            self.loss_D_fake += self.GANcriterion(fake_preds, False)\n",
    "\n",
    "            real_image = torch.cat([self.L[:, i, ...], self.ab[:, i, ...]], dim=1)\n",
    "            real_preds = self.net_D(real_image)\n",
    "            self.loss_D_real += self.GANcriterion(real_preds, True)\n",
    "\n",
    "        self.loss_D_fake /= self.t\n",
    "        self.loss_D_real /= self.t\n",
    "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
    "        self.loss_D.backward()\n",
    "\n",
    "    def backward_G(self):\n",
    "        self.loss_G_GAN = 0\n",
    "        self.loss_G_L1 = 0\n",
    "\n",
    "        for i in range(self.t):\n",
    "            fake_image = torch.cat([self.L[:, i, ...], self.ab_data[:, i, ...]], dim=1)\n",
    "            fake_preds = self.net_D(fake_image)\n",
    "            self.loss_G_GAN += self.GANcriterion(fake_preds, True)\n",
    "            self.loss_G_L1 += self.L1criterion(self.ab_data[:, i, ...], self.ab[:, i, ...]) * self.lambda_L1\n",
    "\n",
    "        self.loss_G_GAN /= self.t\n",
    "        self.loss_G_L1 /= self.t\n",
    "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
    "        self.loss_G.backward()\n",
    "\n",
    "    def optimize(self):\n",
    "        self.forward()\n",
    "        self.net_D.train()\n",
    "        self.set_requires_grad(self.net_D, True)\n",
    "        self.opt_D.zero_grad()\n",
    "        self.backward_D()\n",
    "        self.opt_D.step()\n",
    "\n",
    "        self.net_G.train()\n",
    "        self.set_requires_grad(self.net_D, False)\n",
    "        self.opt_G.zero_grad()\n",
    "        self.backward_G()\n",
    "        self.opt_G.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.count, self.avg, self.sum = [0.] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += count * val\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def create_loss_meters():\n",
    "    loss_D_fake = AverageMeter()\n",
    "    loss_D_real = AverageMeter()\n",
    "    loss_D = AverageMeter()\n",
    "    loss_G_GAN = AverageMeter()\n",
    "    loss_G_L1 = AverageMeter()\n",
    "    loss_G = AverageMeter()\n",
    "\n",
    "    return {'loss_D_fake': loss_D_fake,\n",
    "            'loss_D_real': loss_D_real,\n",
    "            'loss_D': loss_D,\n",
    "            'loss_G_GAN': loss_G_GAN,\n",
    "            'loss_G_L1': loss_G_L1,\n",
    "            'loss_G': loss_G}\n",
    "\n",
    "def update_losses(model, loss_meter_dict, count):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        loss = getattr(model, loss_name)\n",
    "        loss_meter.update(loss.item(), count=count)\n",
    "\n",
    "def lab_to_rgb(L, ab):\n",
    "    \"\"\"\n",
    "    Takes a batch of images\n",
    "    \"\"\"\n",
    "\n",
    "    L = (L + 1.) * 50.\n",
    "    ab = ab * 110.\n",
    "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
    "    rgb_imgs = []\n",
    "    for img in Lab:\n",
    "        img_rgb = lab2rgb(img)\n",
    "        rgb_imgs.append(img_rgb)\n",
    "    return np.stack(rgb_imgs, axis=0)\n",
    "\n",
    "def visualize(model, data, save=True):\n",
    "    model.net_G.eval()\n",
    "    with torch.no_grad():\n",
    "        model.setup_input(data)\n",
    "        model.forward()\n",
    "    model.net_G.train()\n",
    "    fake_color = model.fake_color.detach()\n",
    "    real_color = model.ab\n",
    "    L = model.L\n",
    "    fake_imgs = lab_to_rgb(L, fake_color)\n",
    "    real_imgs = lab_to_rgb(L, real_color)\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    for i in range(5):\n",
    "        ax = plt.subplot(3, 5, i + 1)\n",
    "        ax.imshow(L[i][0].cpu(), cmap='gray')\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(3, 5, i + 1 + 5)\n",
    "        ax.imshow(fake_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(3, 5, i + 1 + 10)\n",
    "        ax.imshow(real_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "    if save:\n",
    "        fig.savefig(f\"colorization_{time.time()}.png\")\n",
    "\n",
    "def log_results(loss_meter_dict):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        print(f\"{loss_name}: {loss_meter.avg:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, epochs, display_every=200):\n",
    "    data = next(iter(val_dl)) # getting a batch for visualizing the model output after fixed intrvals\n",
    "    for e in range(epochs):\n",
    "        loss_meter_dict = create_loss_meters() # function returing a dictionary of objects to\n",
    "        i = 0                                  # log the losses of the complete network\n",
    "        for data in tqdm(train_dl):\n",
    "            model.setup_input(data)\n",
    "            model.optimize()\n",
    "            update_losses(model, loss_meter_dict, count=data['L'].size(0)) # function updating the log objects\n",
    "            i += 1\n",
    "            if i % display_every == 0:\n",
    "                print(f\"\\nEpoch {e+1}/{epochs}\")\n",
    "                print(f\"Iteration {i}/{len(train_dl)}\")\n",
    "                log_results(loss_meter_dict) # function to print out the losses\n",
    "                visualize(model, data, save=False) # function displaying the model's outputs\n",
    "\n",
    "#model = MainModel()\n",
    "#train_model(model, train_dl, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_generator(net_G, train_dl, opt, criterion, epochs, display_every=200):\n",
    "    for e in range(epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        for data in tqdm(train_dl):\n",
    "            L, ab = data['L'].to(device), data['ab'].to(device)\n",
    "            \n",
    "            n, t, c, lr_h, lr_w = L.size()\n",
    "            ab_data = []\n",
    "            \n",
    "            preds = net_G(torch.unsqueeze(L[:, 0, ...],dim=1))\n",
    "            \n",
    "            \n",
    "            ab_data.append(preds['stm'])\n",
    "\n",
    "            # compute the remaining hr data\n",
    "            for i in range(1, t):\n",
    "                ab_curr = net_G(L[:, i, ...], ab_data[i-1])\n",
    "                ab_data.append(ab_curr['stm'])\n",
    "\n",
    "            ab_data = torch.stack(ab_data, dim=1)\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            for i in range(t):\n",
    "                loss += criterion(ab_data[:, i, ...], ab[:, i, ...])\n",
    "\n",
    "            loss /= t\n",
    "                \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            loss_meter.update(loss.item(), L.size(0))\n",
    "\n",
    "            if e % display_every == 0:\n",
    "                print(f\"\\nEpoch {e+1}/{epochs}\")\n",
    "                print(f\"Iteration {e}/{len(train_dl)}\")\n",
    "                visualize(net_G, data, save=False) # function displaying the model's outputs\n",
    "\n",
    "\n",
    "        print(f\"Epoch {e + 1}/{epochs}\")\n",
    "        print(f\"L1 Loss: {loss_meter.avg:.5f}\")\n",
    "\n",
    "#net_G = build_res_unet(n_input=1, n_output=2, size=256)\n",
    "net_G=ColorNet()\n",
    "opt = torch.optim.Adam(net_G.parameters(), lr=1e-4)\n",
    "criterion = nn.L1Loss()\n",
    "pretrain_generator(net_G, train_loader, opt, criterion, 100)\n",
    "torch.save(net_G.state_dict(), \"res18-unet.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
